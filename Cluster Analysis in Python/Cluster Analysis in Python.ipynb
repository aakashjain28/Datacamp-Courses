{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 1 - Introduction to Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pokémon sightings\n",
    "\n",
    "There have been reports of sightings of rare, legendary Pokémon. You have been asked to investigate! Plot the coordinates of sightings to find out where the Pokémon might be. The X and Y coordinates of the points are stored in list `x` and `y`, respectively.\n",
    "\n",
    "### Instructions\n",
    "* Import the `pyplot` class from `matplotlib` library as `plt`.\n",
    "* Create a scatter plot using the `pyplot` class.\n",
    "* Display the scatter plot created in the earlier step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting class from matplotlib library\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Display the scatter plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pokémon sightings: hierarchical clustering\n",
    "\n",
    "We are going to continue the investigation into the sightings of legendary Pokémon from the previous exercise. Remember that in the scatter plot of the previous exercise, you identified two areas where Pokémon sightings were dense. This means that the points seem to separate into two clusters. In this exercise, you will form two clusters of the sightings using hierarchical clustering.\n",
    "\n",
    "`'x'` and `'y'` are columns of X and Y coordinates of the locations of sightings, stored in a Pandas data frame, `df`. The following are available for use: `matplotlib.pyplot` as `plt`, `seaborn` as `sns`, and `pandas` as `pd`.\n",
    "\n",
    "### Instructions\n",
    "* Import the `linkage` and `fcluster` libraries.\n",
    "* Use the `linkage()` function to compute distances using the `ward` method.\n",
    "* Generate cluster labels for each data point with two clusters using the `fcluster()` function.\n",
    "* Plot the points with `seaborn` and assign a different color to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import linkage and fcluster functions\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Use the linkage() function to compute distance\n",
    "Z = linkage(df, 'ward')\n",
    "\n",
    "# Generate cluster labels\n",
    "df['cluster_labels'] = fcluster(Z, 2, criterion='maxclust')\n",
    "\n",
    "# Plot the points with seaborn\n",
    "sns.scatterplot(x='x', y='y', hue='cluster_labels', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize basic list data\n",
    "\n",
    "Now that you are aware of normalization, let us try to normalize some data. goals_for is a list of goals scored by a football team in their last ten matches. Let us standardize the data using the `whiten()` function.\n",
    "\n",
    "### Instructions\n",
    "* Import the `whiten` function.\n",
    "* Use the `whiten()` function to standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the whiten function\n",
    "from scipy.cluster.vq import whiten\n",
    "\n",
    "goals_for = [4,3,2,3,1,1,2,0,1,4]\n",
    "\n",
    "# Use the whiten() function to standardize the data\n",
    "scaled_data = whiten(goals_for)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize normalized data\n",
    "\n",
    "After normalizing your data, you can compare the scaled data to the original data to see the difference. The variables from the last exercise, `goals_for` and `scaled_data` are already available to you.\n",
    "\n",
    "### Instructions\n",
    "* Use the `matplotlib` library to plot the original and scaled data.\n",
    "* Show the legend in the plot.\n",
    "* Display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original data\n",
    "plt.plot(goals_for, label='original')\n",
    "\n",
    "# Plot scaled data\n",
    "plt.plot(scaled_data, label='scaled')\n",
    "\n",
    "# Show the legend in the plot\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of small numbers\n",
    "\n",
    "In earlier examples, you have normalization of whole numbers. In this exercise, you will look at the treatment of fractional numbers - the change of interest rates in the country of Bangalla over the years. For your use, `matplotlib.pyplot` is imported as `plt`.\n",
    "\n",
    "### Instructions\n",
    "* Scale the list `rate_cuts`, which contains the changes in interest rates.\n",
    "* Plot the original data against the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "rate_cuts = [0.0025, 0.001, -0.0005, -0.001, -0.0005, 0.0025, -0.001, -0.0015, -0.001, 0.0005]\n",
    "\n",
    "# Use the whiten() function to standardize the data\n",
    "scaled_data = whiten(rate_cuts)\n",
    "\n",
    "# Plot original data\n",
    "plt.plot(rate_cuts, label='original')\n",
    "\n",
    "# Plot scaled data\n",
    "plt.plot(scaled_data, label='scaled')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIFA 18: Normalize data\n",
    "\n",
    "FIFA 18 is a football video game that was released in 2017 for PC and consoles. The dataset that you are about to work on contains data on the 1000 top individual players in the game. You will explore various features of the data as we move ahead in the course. In this exercise, you will work with two columns, `eur_wage`, the wage of a player in Euros and `eur_value`, their current transfer market value.\n",
    "\n",
    "The data for this exercise is stored in a Pandas dataframe, `fifa`. `whiten` from `scipy.cluster.vq` and `matplotlib.pyplot` as `plt` have been pre-loaded.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Scale the values of eur_wage and eur_value using the `whiten()` function.\n",
    "\n",
    "#### Section 2\n",
    "* Plot the scaled wages and transfer values of players using the `.plot()` method of Pandas.\n",
    "\n",
    "#### Section 3\n",
    "* Check the mean and standard deviation of the scaled data using the `.describe()` method of Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale wage and value\n",
    "fifa['scaled_wage'] = whiten(fifa['eur_wage'])\n",
    "fifa['scaled_value'] = whiten(fifa['eur_value'])\n",
    "\n",
    "# Plot the two columns in a scatter plot\n",
    "fifa.plot(x='scaled_wage', y='scaled_value', kind = 'scatter')\n",
    "plt.show()\n",
    "\n",
    "# Check mean and standard deviation of scaled values\n",
    "print(fifa[['scaled_wage', 'scaled_value']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 2 - Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering: ward method\n",
    "\n",
    "It is time for Comic-Con! Comic-Con is an annual comic-based convention held in major cities in the world. You have the data of last year's footfall, the number of people at the convention ground at a given time. You would like to decide the location of your stall to maximize sales. Using the ward method, apply hierarchical clustering to find the two points of attraction in the area.\n",
    "\n",
    "The data is stored in a Pandas data frame, `comic_con`. `x_scaled` and `y_scaled` are the column names of the standardized X and Y coordinates of people at a given point in time.\n",
    "\n",
    "### Instructions\n",
    "* Import `fcluster` and `linkage` from `scipy.cluster.hierarchy.\n",
    "* Use the `ward` method in the `linkage()` function.\n",
    "* Assign cluster labels by forming 2 flat clusters from `distance_matrix`.\n",
    "* Run the plotting code to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the fcluster and linkage functions\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "# Use the linkage() function\n",
    "distance_matrix = linkage(comic_con[['x_scaled', 'y_scaled']], method = 'ward', metric = 'euclidean')\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'] = fcluster(distance_matrix, 2, criterion='maxclust')\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering: single method\n",
    "\n",
    "Let us use the same footfall dataset and check if any changes are seen if we use a different method for clustering.\n",
    "\n",
    "The data is stored in a Pandas data frame, `comic_con`. `x_scaled` and `y_scaled` are the column names of the standardized X and Y coordinates of people at a given point in time.\n",
    "\n",
    "### Instructions\n",
    "* Import `fcluster` and `linkage` from `scipy.cluster.hierarchy`.\n",
    "* Use the `single` method in the `linkage()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the fcluster and linkage functions\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "# Use the linkage() function\n",
    "distance_matrix = linkage(comic_con[['x_scaled', 'y_scaled']], method = 'single', metric = 'euclidean')\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'] = fcluster(distance_matrix, 2, criterion='maxclust')\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering: complete method\n",
    "\n",
    "For the third and final time, let us use the same footfall dataset and check if any changes are seen if we use a different method for clustering.\n",
    "\n",
    "The data is stored in a Pandas data frame, `comic_con`. `x_scaled` and `y_scaled` are the column names of the standardized X and Y coordinates of people at a given point in time.\n",
    "\n",
    "### Instructions\n",
    "* Import `fcluster` and `linkage` from `scipy.cluster.hierarchy`.\n",
    "* Use the complete method in the `.linkage()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the fcluster and linkage functions\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "# Use the linkage() function\n",
    "distance_matrix = linkage(comic_con[['x_scaled', 'y_scaled']], method='complete', metric='euclidean')\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'] = fcluster(distance_matrix, 2, criterion='maxclust')\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize clusters with matplotlib\n",
    "\n",
    "We have discussed that visualizations are necessary to assess the clusters that are formed and spot trends in your data. Let us now focus on visualizing the footfall dataset from Comic-Con using the `matplotlib` module.\n",
    "\n",
    "The data is stored in a Pandas data frame, `comic_con`. `x_scaled` and `y_scaled` are the column names of the standardized X and Y coordinates of people at a given point in time. `cluster_labels` has the cluster labels. A linkage object is stored in the variable `distance_matrix`.\n",
    "\n",
    "### Instructions\n",
    "* Import the `pyplot` class from `matplotlib` module as `plt`.\n",
    "* Define a colors dictionary for two cluster labels, 1 and 2.\n",
    "* Plot a scatter plot with colors for each cluster as defined by the colors dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyplot class\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Define a colors dictionary for clusters\n",
    "colors = {1:'red', 2:'blue'}\n",
    "\n",
    "# Plot a scatter plot\n",
    "comic_con.plot.scatter(x='x_scaled', \n",
    "                \t   y='y_scaled',\n",
    "                \t   c=comic_con['cluster_labels'].apply(lambda x: colors[x]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize clusters with seaborn\n",
    "\n",
    "Let us now visualize the footfall dataset from Comic Con using the `seaborn` module. Visualizing clusters using `seaborn` is easier with the inbuild hue function for cluster labels.\n",
    "\n",
    "The data is stored in a Pandas data frame, `comic_con`. `x_scaled` and `y_scaled` are the column names of the standardized X and Y coordinates of people at a given point in time. `cluster_labels` has the cluster labels. A linkage object is stored in the variable `distance_matrix`.\n",
    "\n",
    "### Instructions\n",
    "* Import the `seaborn` module as `sns`.\n",
    "* Plot a scatter plot using the `.scatterplot()` method of `seaborn`, with the cluster labels as the `hue` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the seaborn module\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot a scatter plot using seaborn\n",
    "sns.scatterplot(x='x_scaled', \n",
    "                y='y_scaled', \n",
    "                hue='cluster_labels', \n",
    "                data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dendrogram\n",
    "\n",
    "Dendrograms are branching diagrams that show the merging of clusters as we move through the distance matrix. Let us use the Comic Con footfall data to create a dendrogram.\n",
    "\n",
    "The data is stored in a Pandas data frame, `comic_con`. `x_scaled` and `y_scaled` are the column names of the standardized X and Y coordinates of people at a given point in time. `cluster_labels` has the cluster labels. A linkage object is stored in the variable `distance_matrix`.\n",
    "\n",
    "### Instructions\n",
    "* Import the `dendrogram` function from `scipy.cluster.hierarchy`.\n",
    "* Create a dendrogram using the linkage object.\n",
    "* Display the dendrogram using `.show()` method of the `plt` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dendrogram function\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Create a dendrogram\n",
    "dn = dendrogram(distance_matrix)\n",
    "\n",
    "# Display the dendogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIFA 18: exploring defenders\n",
    "\n",
    "In the FIFA 18 dataset, various attributes of players are present. Two such attributes are:\n",
    "* sliding tackle: a number between 0-99 which signifies how accurate a player is able to perform sliding tackles\n",
    "* aggression: a number between 0-99 which signifies the commitment and will of a player\n",
    "\n",
    "These are typically high in defense-minded players. In this exercise, you will perform clustering based on these attributes in the data.\n",
    "\n",
    "_This data consists of 5000 rows, and is considerably larger than earlier datasets. Running hierarchical clustering on this data can take up to 10 seconds._\n",
    "\n",
    "The following modules are pre-loaded: `dendrogram`, `linkage`, `fcluster` from `scipy.cluster.hierarchy`, `matplotlib.pyplot` as `plt`, `seaborn` as `sns`. The data is stored in a Pandas dataframe, `fifa`    .\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Fit the scaled data in columns `scaled_sliding_tackle` and `scaled_aggression` into a hierarchical clustering algorithm. Additionally, you may want to check how long it takes to run the data in the console using the `timeit` module.\n",
    "\n",
    "#### Section 2\n",
    "* Assign cluster labels to each row in the data using the `fcluster()` function (use 3 clusters).\n",
    "\n",
    "#### Section 3\n",
    "* Display cluster centers of each cluster with respect to the scaled columns by calculating the mean value for each cluster.\n",
    "\n",
    "#### Section 4\n",
    "* Create a scatter plot using `seaborn` with the `scaled_sliding_tackle` attribute on the x-axis and the `scaled_aggression` attribute on the y-axis. Assign a different color to each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data into a hierarchical clustering algorithm\n",
    "distance_matrix = linkage(fifa[['scaled_sliding_tackle', 'scaled_aggression']], 'ward')\n",
    "\n",
    "# Assign cluster labels to each row of data\n",
    "fifa['cluster_labels'] = fcluster(distance_matrix, 3, criterion='maxclust')\n",
    "\n",
    "# Display cluster centers of each cluster\n",
    "print(fifa[['scaled_sliding_tackle', 'scaled_aggression', 'cluster_labels']].groupby('cluster_labels').mean())\n",
    "\n",
    "# Create a scatter plot through seaborn\n",
    "sns.scatterplot(x='scaled_sliding_tackle', y='scaled_aggression', hue='cluster_labels', data=fifa)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 3 - K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering: first exercise\n",
    "\n",
    "This exercise will familiarize you with the usage of k-means clustering on a dataset. Let us use the Comic Con dataset and check how k-means clustering works on it.\n",
    "\n",
    "Recall the two steps of k-means clustering:\n",
    "* Define cluster centers through `kmeans()` function. It has two required arguments: observations and number of clusters.\n",
    "* Assign cluster labels through the `vq()` function. It has two required arguments: observations and cluster centers.\n",
    "\n",
    "The data is stored in a Pandas data frame, `comic_con`. `x_scaled` and `y_scaled` are the column names of the standardized X and Y coordinates of people at a given point in time.\n",
    "\n",
    "### Instructions\n",
    "* Import `kmeans` and `vq` functions in SciPy.\n",
    "* Generate cluster centers using the `kmeans()` function with two clusters.\n",
    "* Create cluster labels using these cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(comic_con[['x_scaled', 'y_scaled']], 2)\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'], distortion_list = vq(comic_con[['x_scaled', 'y_scaled']], cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow method on distinct clusters\n",
    "\n",
    "Let us use the comic con data set to see how the elbow plot looks on a data set with distinct, well-defined clusters. You may want to display the data points before proceeding with the exercise.\n",
    "\n",
    "The data is stored in a Pandas data frame, `comic_con`. `x_scaled` and `y_scaled` are the column names of the standardized X and Y coordinates of people at a given point in time.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Create a list of distortions for each cluster in `num_clusters`.\n",
    "* Create a data frame `elbow_plot` with `num_clusters` and `distortions`.\n",
    "* With the `.lineplot()` method, plot `elbow_plot` with `num_clusters` in the x axis and `distortions` in the y axis.\n",
    "\n",
    "#### Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "num_clusters = range(1, 7)\n",
    "\n",
    "# Create a list of distortions from the kmeans function\n",
    "for i in num_clusters:\n",
    "    cluster_centers, distortion = kmeans(comic_con[['x_scaled', 'y_scaled']], i)\n",
    "    distortions.append(distortion)\n",
    "\n",
    "# Create a data frame with two lists - num_clusters, distortions\n",
    "elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})\n",
    "\n",
    "# Creat a line plot of num_clusters and distortions\n",
    "sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)\n",
    "plt.xticks(num_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow method on uniform data\n",
    "\n",
    "In the earlier exercise, you constructed an elbow plot on data with well-defined clusters. Let us now see how the elbow plot looks on a data set with uniformly distributed points. You may want to display the data points on the console before proceeding with the exercise.\n",
    "\n",
    "The data is stored in a Pandas data frame, uniform_data. `x_scaled` and `y_scaled` are the column names of the standardized X and Y coordinates of points.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Create a list of distortions for each cluster in `num_clusters`.\n",
    "* Create a data frame `elbow_plot` with `num_clusters` and `distortions`.\n",
    "* With the `.lineplot()` method, plot `elbow_plot` with `num_clusters` in the x axis and `distortions` in the y axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of seeds on distinct clusters\n",
    "\n",
    "You noticed the impact of seeds on a dataset that did not have well-defined groups of clusters. In this exercise, you will explore whether seeds impact the clusters in the Comic Con data, where the clusters are well-defined.\n",
    "\n",
    "The data is stored in a Pandas data frame, `comic_con`. `x_scaled` and `y_scaled` are the column names of the standardized X and Y coordinates of people at a given point in time.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Import the `random` class from `numpy` and initialize the seed with the integer 0.\n",
    "\n",
    "#### Section 2\n",
    "* Change your code from the earlier step so that the seed is initialized with a list `[1, 2, 1000]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import random class\n",
    "from numpy import random\n",
    "\n",
    "# Initialize seed\n",
    "random.seed(0)\n",
    "\n",
    "# Run kmeans clustering\n",
    "cluster_centers, distortion = kmeans(comic_con[['x_scaled', 'y_scaled']], 2)\n",
    "comic_con['cluster_labels'], distortion_list = vq(comic_con[['x_scaled', 'y_scaled']], cluster_centers)\n",
    "\n",
    "# Plot the scatterplot\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()\n",
    "\n",
    "# Initialize seed\n",
    "random.seed([1, 2, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform clustering patterns\n",
    "\n",
    "Now that you are familiar with the impact of seeds, let us look at the bias in k-means clustering towards the formation of uniform clusters.\n",
    "\n",
    "Let us use a mouse-like dataset for our next exercise. A mouse-like dataset is a group of points that resemble the head of a mouse: it has three clusters of points arranged in circles, one each for the face and two ears of a mouse.\n",
    "\n",
    "Here is how a typical mouse-like dataset looks like ([Source](https://www.researchgate.net/figure/Clustering-results-for-the-Mouse-data-set-where-the-black-boxes-represent-the-centroids_fig3_256378655)).\n",
    "\n",
    "![Mouse-shaped data set](https://assets.datacamp.com/production/repositories/3842/datasets/fa03a65258018a0c945528a987cdd250010de1ee/Clustering-results-for-the-Mouse-data-set-where-the-black-boxes-represent-the-centroids.ppm)\n",
    "\n",
    "The data is stored in a Pandas data frame, `mouse`. `x_scaled` and `y_scaled` are the column names of the standardized X and Y coordinates of the data points.\n",
    "\n",
    "### Instructions\n",
    "* Import `kmeans` and `vq` functions in SciPy.\n",
    "* Generate cluster centers using the `kmeans()` function with three clusters.\n",
    "* Create cluster labels with `vq()` with the cluster centers generated above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(mouse[['x_scaled', 'y_scaled']], 3)\n",
    "\n",
    "# Assign cluster labels\n",
    "mouse['cluster_labels'], distortion_list = vq(mouse[['x_scaled', 'y_scaled']], cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = mouse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIFA 18: defenders revisited\n",
    "\n",
    "In the FIFA 18 dataset, various attributes of players are present. Two such attributes are:\n",
    "* defending: a number which signifies the defending attributes of a player\n",
    "* physical: a number which signifies the physical attributes of a player\n",
    "\n",
    "These are typically defense-minded players. In this exercise, you will perform clustering based on these attributes in the data.\n",
    "\n",
    "The following modules have been pre-loaded: `kmeans`, `vq` from `scipy.cluster.vq`, `matplotlib.pyplot` as `plt`, `seaborn` as `sns`. The data for this exercise is stored in a Pandas dataframe, `fifa`. The scaled variables are `scaled_def` and `scaled_phy`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Initialize the random seed to the list [1000,2000].\n",
    "\n",
    "#### Section 2\n",
    "* Fit the scaled data in columns `scaled_def` and `scaled_phy` into a k-means clustering algorithm with 3 clusters and assign cluster labels.\n",
    "\n",
    "#### Section 3\n",
    "* Display cluster centers of each cluster with respect to the scaled columns by calculating the mean value for each cluster.\n",
    "\n",
    "#### Section 4\n",
    "* Create a seaborn scatter plot with `scaled_def` on the x-axis and `scaled_phy` on the y-axis, with each cluster represented by a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a random seed in numpy\n",
    "random.seed([1000,2000])\n",
    "\n",
    "# Fit the data into a k-means algorithm\n",
    "cluster_centers,_ = kmeans(fifa[['scaled_def', 'scaled_phy']], 3)\n",
    "\n",
    "# Assign cluster labels\n",
    "fifa['cluster_labels'], _ = vq(fifa[['scaled_def', 'scaled_phy']], cluster_centers)\n",
    "\n",
    "# Display cluster centers \n",
    "print(fifa[['scaled_def', 'scaled_phy', 'cluster_labels']].groupby('cluster_labels').mean())\n",
    "\n",
    "# Create a scatter plot through seaborn\n",
    "sns.scatterplot(x='scaled_def', y='scaled_phy', hue='cluster_labels', data=fifa)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 4 - Clustering in Real World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract RGB values from image\n",
    "\n",
    "There are broadly three steps to find the dominant colors in an image:\n",
    "* Extract RGB values into three lists.\n",
    "* Perform k-means clustering on scaled RGB values.\n",
    "* Display the colors of cluster centers.\n",
    "\n",
    "To extract RGB values, we use the `imread()` function of the image class of `matplotlib`. Empty lists, r, g and b have been initialized.\n",
    "\n",
    "For the purpose of finding dominant colors, we will be using the following image.\n",
    "\n",
    "![Batman-Robin](https://assets.datacamp.com/production/repositories/3842/datasets/57d0d6d409bfd543e86c7f7398239fa0722e9b48/batman.jpg)\n",
    "\n",
    "### Instructions\n",
    "* Import `image` class of `matplotlib`.\n",
    "* Read the image using the `imread()` function and print the dimensions of the resultant matrix.\n",
    "* Store the values for the three colors from all pixels in lists `r`, `g` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image class of matplotlib\n",
    "import matplotlib.image as img\n",
    "\n",
    "# Read batman image and print dimensions\n",
    "batman_image = img.imread('batman.jpg')\n",
    "print(batman_image.shape)\n",
    "\n",
    "# Store RGB values of all pixels in lists r, g and b\n",
    "for row in batman_image:\n",
    "    for temp_r, temp_g, temp_b in row:\n",
    "        r.append(temp_r)\n",
    "        g.append(temp_g)\n",
    "        b.append(temp_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many dominant colors?\n",
    "\n",
    "We have loaded the following image using the `imread()` function of the `image` class of `matplotlib`.\n",
    "\n",
    "The RGB values are stored in a data frame, `batman_df`. The RGB values have been standardized used the `whiten()` function, stored in columns, `scaled_red`, `scaled_blue` and `scaled_green`.\n",
    "\n",
    "Construct an elbow plot with the data frame. How many dominant colors are present?\n",
    "\n",
    "### Instructions\n",
    "* Create a list of distortions based on each value in `num_clusters` by running the `kmeans()` function.\n",
    "* Create a data frame `elbow_plot` with the lists: `num_clusters` and `distortions`.\n",
    "* Plot the data with seaborn's `.lineplot()` method with `num_clusters` on the x-axis and `distortions` on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "num_clusters = range(1, 7)\n",
    "\n",
    "# Create a list of distortions from the kmeans function\n",
    "for i in num_clusters:\n",
    "    cluster_centers, distortion = kmeans(batman_df[['scaled_red', 'scaled_blue', 'scaled_green']], i)\n",
    "    distortions.append(distortion)\n",
    "\n",
    "# Create a data frame with two lists, num_clusters and distortions\n",
    "elbow_plot = pd.DataFrame({'num_clusters': num_clusters,\n",
    "                           'distortions': distortions})\n",
    "\n",
    "# Create a line plot of num_clusters and distortions\n",
    "sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)\n",
    "plt.xticks(num_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display dominant colors\n",
    "\n",
    "We have loaded the following image using the `imread()` function of the `image` class of `matplotlib`.\n",
    "\n",
    "To display the dominant colors, convert the colors of the cluster centers to their raw values and then convert them to the range of 0-1, using the following formula: `converted_pixel = standardized_pixel * pixel_std / 255`\n",
    "\n",
    "The RGB values are stored in a data frame, `batman_df`. The scaled RGB values are stored in columns, `scaled_red`, `scaled_blue` and `scaled_green`. The cluster centers are stored in the variable `cluster_centers`, which were generated using the `kmeans()` function with three clusters.\n",
    "\n",
    "### Instructions\n",
    "* Get standard deviations of each color from the data frame and store it in `r_std`, `g_std`, `b_std`.\n",
    "* For each cluster center, convert the standardized RGB values to scaled values in the range of 0-1.\n",
    "* Display the colors of the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standard deviations of each color\n",
    "r_std, g_std, b_std = batman_df[['red', 'green', 'blue']].std()\n",
    "\n",
    "for cluster_center in cluster_centers:\n",
    "    scaled_r, scaled_g, scaled_b = cluster_center\n",
    "    # Convert each standardized value to scaled value\n",
    "    colors.append((\n",
    "        scaled_r * r_std / 255,\n",
    "        scaled_g * g_std / 255,\n",
    "        scaled_b * b_std / 255\n",
    "    ))\n",
    "\n",
    "# Display colors of cluster centers\n",
    "plt.imshow([colors])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF of movie plots\n",
    "\n",
    "Let us use the plots of randomly selected movies to perform document clustering on. Before performing clustering on documents, they need to be cleaned of any unwanted noise (such as special characters and stop words) and converted into a sparse matrix through TF-IDF of the documents.\n",
    "\n",
    "Use the `TfidfVectorizer` class to perform the TF-IDF of movie plots stored in the list `plots`. The `remove_noise()` function is available to use as a tokenizer in the `TfidfVectorizer` class. The `.fit_transform()` method fits the data into the `TfidfVectorizer` objects and then generates the TF-IDF sparse matrix.\n",
    "\n",
    "Note: It takes a few seconds to run the `.fit_transform()` method.\n",
    "\n",
    "### Instructions\n",
    "* Import `TfidfVectorizer` class from `sklearn`.\n",
    "* Initialize the `TfidfVectorizer` class with minimum and maximum frequencies of 0.1 and 0.75, and 50 maximum features.\n",
    "* Use the `fit_transform()` method on the initialized `TfidfVectorizer` class with the list `plots`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer class from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.75, min_df=0.1, max_features=50, tokenizer=remove_noise)\n",
    "\n",
    "# Use the .fit_transform() method on the list plots\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top terms in movie clusters\n",
    "\n",
    "Now that you have created a sparse matrix, generate cluster centers and print the top three terms in each cluster. Use the `.todense()` method to convert the sparse matrix, `tfidf_matrix` to a normal matrix for the `kmeans()` function to process. Then, use the `.get_feature_names()` method to get a list of terms in the `tfidf_vectorizer` object. The `zip()` function in Python joins two lists.\n",
    "\n",
    "The `tfidf_vectorizer` object and sparse matrix, `tfidf_matrix`, from the previous have been retained in this exercise. `kmeans` has been imported from SciPy.\n",
    "\n",
    "With a higher number of data points, the clusters formed would be defined more clearly. However, this requires some computational power, making it difficult to accomplish in an exercise here.\n",
    "\n",
    "### Instructions\n",
    "* Generate cluster centers through the `kmeans()` function.\n",
    "* Generate a list of terms from the `tfidf_vectorizer` object.\n",
    "* Print top 3 terms of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "\n",
    "# Generate cluster centers through the kmeans function\n",
    "cluster_centers, distortion = kmeans(tfidf_matrix.todense(), num_clusters)\n",
    "\n",
    "# Generate terms from the tfidf_vectorizer object\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    # Sort the terms and print top 3 terms\n",
    "    center_terms = dict(zip(terms, list(cluster_centers[i])))\n",
    "    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n",
    "    print(sorted_terms[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic checks on clusters\n",
    "\n",
    "In the FIFA 18 dataset, we have concentrated on defenders in previous exercises. Let us try to focus on attacking attributes of a player. Pace (`pac`), Dribbling (`dri`) and Shooting (`sho`) are features that are present in attack minded players. In this exercise, k-means clustering has already been applied on the data using the scaled values of these three attributes. Try some basic checks on the clusters so formed.\n",
    "\n",
    "The data is stored in a Pandas data frame, `fifa`. The scaled column names are present in a list `scaled_features`. The cluster labels are stored in the `cluster_labels` column. Recall the `.count()` and `.mean()` methods in Pandas help you find the number of observations and mean of observations in a data frame.\n",
    "\n",
    "### Instructions\n",
    "* Print the size of the clusters by grouping the column `cluster_labels`.\n",
    "* Print the mean values of the wages of the players in each cluster. `eur_wage` is the column name that stores the wages of a player in Euros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the size of the clusters\n",
    "print(fifa.groupby('cluster_labels')['ID'].count())\n",
    "\n",
    "# Print the mean value of wages in each cluster\n",
    "print(fifa.groupby('cluster_labels')['eur_wage'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIFA 18: what makes a complete player?\n",
    "\n",
    "The overall level of a player in FIFA 18 is defined by six characteristics: pace (`pac`), shooting (`sho`), passing (`pas`), dribbling (`dri`), defending (`def`), physical (`phy`).\n",
    "\n",
    "Here is a sample card:\n",
    "![FIFA Sample Card](https://media.contentapi.ea.com/content/dam/ea/easports/fifa/features/2017/top100-ratings/10-1/fut18-top100-hazard-lg.jpg)\n",
    "\n",
    "In this exercise, you will use all six characteristics to create clusters. The data for this exercise is stored in a Pandas dataframe, `fifa`. `features` is the list of these column names and `scaled_features` is the list of columns which contains their scaled values. The following have been pre-loaded: `kmeans`, `vq` from `scipy.cluster.vq`, `matplotlib.pyplot` as `plt`, `seaborn` as `sns`.\n",
    "\n",
    "Before you start the exercise, you may wish to explore `scaled_features` in the console to check out the list of six scaled columns names.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Use the `kmeans()` algorithm to create 2 clusters using the list of columns, `scaled_features`.\n",
    "\n",
    "#### Section 2\n",
    "* Assign cluster labels to each row using `vq()` and print cluster centers of `scaled_features` using the `.mean()` method of Pandas.\n",
    "\n",
    "#### Section 3\n",
    "* Plot a bar chart of scaled attributes of each cluster center using the `.plot()` method of Pandas.\n",
    "\n",
    "#### Section 4\n",
    "* Print the names of first 5 players in each cluster, using the `name` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create centroids with kmeans for 2 clusters\n",
    "cluster_centers,_ = kmeans(fifa[scaled_features], 2)\n",
    "\n",
    "# Assign cluster labels and print cluster centers\n",
    "fifa['cluster_labels'], _ = vq(fifa[scaled_features], cluster_centers)\n",
    "print(fifa.groupby('cluster_labels')[scaled_features].mean())\n",
    "\n",
    "# Plot cluster centers to visualize clusters\n",
    "fifa.groupby('cluster_labels')[scaled_features].mean().plot(legend=True, kind='bar')\n",
    "plt.show()\n",
    "\n",
    "# Get the name column of first 5 players in each cluster\n",
    "for cluster in fifa['cluster_labels'].unique():\n",
    "    print(cluster, fifa[fifa['cluster_labels'] == cluster]['name'].values[:5])"
   ]
  }
 ]
}