{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 1 - Regular expressions and word tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practicing regular expressions: `re.split()` and `re.findall()`\n",
    "\n",
    "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at `my_string` first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
    "\n",
    "Note: It's important to prefix your `regex` patterns with `r` to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, `\"\\n\"` in Python is used to indicate a new line, but if you use the `r` prefix, it will be interpreted as the raw string `\"\\n\"` - that is, the character \"\\\" followed by the character \"n\" - and not as a new line.\n",
    "\n",
    "The regular expression module `re` has already been imported for you.\n",
    "\n",
    "Remember from the video that the syntax for the `regex` library is to always to pass the pattern first, and then the string second.\n",
    "### Instructions\n",
    "* Split `my_string` on each sentence ending. To do this:\n",
    "    * Write a pattern called `sentence_endings` to match sentence endings (`.?!`).\n",
    "    * Use `re.split()` to split `my_string` on the pattern and print the result.\n",
    "* Find and print all capitalized words in `my_string` by writing a pattern called `capitalized_words` and using `re.findall()`.\n",
    "    * Remember the `[a-z]` pattern shown in the video to match lowercase groups? Modify that pattern appropriately in order to match uppercase groups.\n",
    "* Write a pattern called `spaces` to match one or more spaces (`\"\\s+\"`) and then use `re.split()` to split `my_string` on this pattern, keeping all punctuation intact. Print the result.\n",
    "* Find all digits in `my_string` by writing a pattern called digits (`\"\\d+\"`) and using `re.findall()`. Print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization with NLTK\n",
    "\n",
    "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as `scene_one`. Feel free to check it out in the IPython Shell!\n",
    "\n",
    "Your job in this exercise is to utilize `word_tokenize` and `sent_tokenize` from `nltk.tokenize` to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail.\n",
    "\n",
    "### Instructions\n",
    "* Import the `sent_tokenize` and `word_tokenize` functions from `nltk.tokenize`.\n",
    "* Tokenize all the sentences in `scene_one` using the `sent_tokenize()` function.\n",
    "* Tokenize the fourth sentence in `sentences`, which you can access as `sentences[3]`, using the `word_tokenize()` function.\n",
    "* Find the unique tokens in the entire scene by using `word_tokenize()` on `scene_one` and then converting it into a set using `set()`.\n",
    "* Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More regex with `re.search()`\n",
    "\n",
    "In this exercise, you'll utilize `re.search()` and `re.match()` to find specific tokens. Both `search` and `match` expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the `nltk` corpora.\n",
    "\n",
    "You have both `scene_one` and `sentences` available from the last exercise; now you can use them with `re.search()` and `re.match()` to extract and match more text.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Use `re.search()` to search for the first occurrence of the word \"coconuts\" in `scene_one`. Store the result in `match`.\n",
    "* Print the start and end indexes of match using its `.start()` and `.end()` methods, respectively.\n",
    "\n",
    "#### Section 2\n",
    "* Write a regular expression called `pattern1` to find anything in square brackets.\n",
    "* Use `re.search()` with the pattern to find the first text in `scene_one` in square brackets in the scene. Print the result.\n",
    "\n",
    "#### Section 3\n",
    "* Create a pattern to match the script notation (e.g. Character:), assigning the result to `pattern2`. Remember that you will want to match any words or spaces that precede the : (such as the space within SOLDIER #1:).\n",
    "* Use `re.match()` with your new pattern to find and print the script notation in the fourth line. The tokenized sentences are available in your namespace as `sentences`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e49e302f9fa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Search for the first occurrence of \"coconuts\" in scene_one: match\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"coconuts\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscene_one\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Print the start and end indexes of match\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w+]+:\"\n",
    "print(re.match(pattern2, sentences[3]))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex with NLTK tokenization\n",
    "\n",
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using `nltk` and `regex`. The `nltk.tokenize.TweetTokenizer` class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "Here, you're given some example tweets to parse using both `TweetTokenizer` and `regexp_tokenize` from the `nltk.tokenize module`. These example tweets have been pre-loaded into the variable `tweets`. Feel free to explore it in the IPython Shell!\n",
    "\n",
    "_Unlike the syntax for the `regex` library, with `nltk_tokenize()` you pass the pattern as the second argument._\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* From `nltk.tokenize`, import `regexp_tokenize` and `TweetTokenizer`.\n",
    "\n",
    "#### Section 2\n",
    "* A regex pattern to define hashtags called `pattern1` has been defined for you. Call `regexp_tokenize()` with this hashtag pattern on the first tweet in tweets and assign the result to `hashtags`.\n",
    "* Print `hashtags` (this has already been done for you).\n",
    "\n",
    "#### Section 3\n",
    "* Write a new pattern called `pattern2` to match mentions and hashtags. A mention is something like `@DataCamp`.\n",
    "* Then, call `regexp_tokenize()` with your new hashtag pattern on the last tweet in `tweets` and assign the result to `mentions_hashtags`.\n",
    "    * You can access the last element of a list using -1 as the index, for example, `tweets[-1]`.\n",
    "* Print `mentions_hashtags` (this has been done for you).\n",
    "\n",
    "#### Section 4\n",
    "* Create an instance of `TweetTokenizer` called `tknzr` and use it inside a list comprehension to tokenize each tweet into a new list called `all_tokens`.\n",
    "    * To do this, use the `.tokenize()` method of `tknzr`, with `t` as your iterator variable.\n",
    "* Print `all_tokens`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)\n",
    "\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@|#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-ascii tokenization\n",
    "\n",
    "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "Here, you have access to a string called `german_text`, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
    "\n",
    "The following modules have been pre-imported from `nltk.tokenize`: `regexp_tokenize` and `word_tokenize`.\n",
    "\n",
    "Unicode ranges for emoji are:\n",
    "\n",
    "`('\\U0001F300'-'\\U0001F5FF')`, `('\\U0001F600-\\U0001F64F')`, `('\\U0001F680-\\U0001F6FF')`, and `('\\u2600'-\\u26FF-\\u2700-\\u27BF')`.\n",
    "\n",
    "### Instructions\n",
    "* Tokenize all the words in `german_text` using `word_tokenize()`, and print the result.\n",
    "* Tokenize only the capital words in `german_text`.\n",
    "    * First, write a pattern called `capital_words` to match only capital words. Make sure to check for the German Ü! To use this character in the exercise, copy and paste it from these instructions.\n",
    "    * Then, tokenize it using `regexp_tokenize()`.\n",
    "* Tokenize only the emoji in `german_text`. The pattern using the unicode ranges for emoji given in the assignment text has been written for you. Your job is to use `regexp_tokenize()` to tokenize the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting practice\n",
    "\n",
    "Try using your new skills to find and chart the number of words per line in the script using `matplotlib`. The Holy Grail script is loaded for you, and you need to use `regex` to find the words per line.\n",
    "\n",
    "Using list comprehensions here will speed up your computations. For example: `my_lines = [tokenize(l) for l in lines]` will call a function `tokenize` on each line in the list lines. The new transformed list will be saved in the `my_lines` variable.\n",
    "\n",
    "You have access to the entire script in the variable `holy_grail`. Go for it!\n",
    "\n",
    "### Instructions\n",
    "* Split the script `holy_grail` into lines using the newline (`'\\n'`) character.\n",
    "* Use `re.sub()` inside a list comprehension to replace the prompts such as `ARTHUR:` and `SOLDIER #1`. The pattern has been written for you.\n",
    "* Use a list comprehension to tokenize lines with `regexp_tokenize()`, keeping only words. Recall that the pattern for words is `\"\\w+\"`.\n",
    "* Use a list comprehension to create a list of line lengths called `line_num_words`.\n",
    "    * Use `t_line` as your iterator variable to iterate over `tokenized_lines`, and then `len()` function to compute line lengths.\n",
    "* Plot a histogram of `line_num_words` using `plt.hist()`. Don't forgot to use `plt.show()` as well to display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Counter with bag-of-words\n",
    "\n",
    "In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as `article_title`. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.\n",
    "\n",
    "`word_tokenize` has been imported for you.\n",
    "\n",
    "### Instructions\n",
    "* Import `Counter` from `collections`.\n",
    "* Use `word_tokenize()` to split the article into tokens.\n",
    "* Use a list comprehension with `t` as the iterator variable to convert all the tokens into lowercase. The `.lower()` method converts text into lowercase.\n",
    "* Create a bag-of-words counter called `bow_simple` by using `Counter()` with `lower_tokens` as the argument.\n",
    "* Use the `.most_common()` method of `bow_simple` to print the 10 most common tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing practice\n",
    "\n",
    "Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n",
    "\n",
    "You start with the same tokens you created in the last exercise: `lower_tokens`. You also have the `Counter` class imported.\n",
    "\n",
    "### Instructions\n",
    "* Import the `WordNetLemmatizer` class from `nltk.stem`.\n",
    "* Create a list `alpha_only` that contains only alphabetical characters. You can use the `.isalpha()` method to check for this.\n",
    "* Create another list called `no_stops` consisting of words from `alpha_only` that are not contained in `english_stops`.\n",
    "* Initialize a `WordNetLemmatizer` object called `wordnet_lemmatizer` and use its `.lemmatize()` method on the tokens in `no_stops` to create a new list called `lemmatized`.\n",
    "* Create a new `Counter` called `bow` with the lemmatized words.\n",
    "* Lastly, print the 10 most common tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and querying a corpus with gensim\n",
    "\n",
    "It's time to apply the methods you learned in the previous video to create your first `gensim` dictionary and corpus!\n",
    "\n",
    "You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called `articles`. You'll need to do some light preprocessing and then generate the `gensim` dictionary and corpus.\n",
    "\n",
    "### Instructions\n",
    "* Import `Dictionary` from `gensim.corpora.dictionary`.\n",
    "* Initialize a `gensim` Dictionary with the tokens in `articles`.\n",
    "* Obtain the id for `\"computer\"` from `dictionary`. To do this, use its `.token2id` method which returns ids from text, and then chain `.get()` which returns tokens from ids. Pass in `\"computer\"` as an argument to `.get()`.\n",
    "* Use a list comprehension in which you iterate over `articles` to create a `gensim` `MmCorpus` from `dictionary`.\n",
    "    * In the output expression, use the `.doc2bow()` method on dictionary with `article` as the argument.\n",
    "* Print the first 10 word `ids` with their frequency counts from the fifth document. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim bag-of-words\n",
    "\n",
    "Now, you'll use your new `gensim` corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell!\n",
    "\n",
    "You have access to the `dictionary` and `corpus` objects you created in the previous exercise, as well as the Python `defaultdict` and `itertools` to help with the creation of intermediate data structures for analysis.\n",
    "\n",
    "* `defaultdict` allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument `int`, we are able to ensure that any non-existent keys are automatically assigned a default value of 0. This makes it ideal for storing the counts of words in this exercise.\n",
    "* `itertools.chain.from_iterable()` allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our `corpus` object (which is a list of lists).\n",
    "\n",
    "The fifth document from `corpus` is stored in the variable `doc`, which has been sorted in descending order.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Using the first for loop, print the top five words of `bow_doc` using each `word_id` with the dictionary alongside `word_count`.\n",
    "    * The `word_id` can be accessed using the `.get()` method of dictionary.\n",
    "* Create a `defaultdict` called `total_word_count` in which the keys are all the token ids (`word_id`) and the values are the sum of their occurrence across all documents (`word_count`).\n",
    "    * Remember to specify int when creating the `defaultdict`, and inside the second for loop, increment each `word_id` of `total_word_count` by `word_count`.\n",
    "\n",
    "#### Section 2\n",
    "* Create a sorted list from the `defaultdict`, using words across the entire corpus. To achieve this, use the `.items()` method on `total_word_count` inside `sorted()`.\n",
    "* Similar to how you printed the top five words of `bow_doc` earlier, print the top five words of `sorted_word_count` as well as the number of occurrences of each word across all the documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tf-idf` with Wikipedia\n",
    "\n",
    "Now it's your turn to determine new significant terms for your corpus by applying `gensim`'s `tf-idf`. You will again have access to the same corpus and dictionary objects you created in the previous exercises - `dictionary`, `corpus`, and `doc`. Will `tf-idf` make for more interesting results on the document level?\n",
    "\n",
    "`TfidfModel` has been imported for you from `gensim.models.tfidfmodel`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Initialize a new `TfidfModel` called `tfidf` using `corpus`.\n",
    "* Use `doc` to calculate the weights. You can do this by passing `[doc]` to `tfidf`.\n",
    "* Print the first five term ids with weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 3 - Named entity reconition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with NLTK\n",
    "\n",
    "You're now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use `nltk` to find the named entities in this article.\n",
    "\n",
    "What might the article be about, given the names you found?\n",
    "\n",
    "Along with `nltk`, `sent_tokenize` and `word_tokenize` from `nltk.tokenize` have been pre-imported.\n",
    "\n",
    "### Instructions\n",
    "* Tokenize `article` into sentences.\n",
    "* Tokenize each sentence in `sentences` into words using a list comprehension.\n",
    "* Inside a list comprehension, tag each tokenized sentence into parts of speech using `nltk.pos_tag()`.\n",
    "* Chunk each tagged sentence into `named-entity` chunks using `nltk.ne_chunk_sents()`. Along with `pos_sentences`, specify the additional keyword argument `binary=True`.\n",
    "* Loop over each sentence and each chunk, and test whether it is a named-entity chunk by testing if it has the attribute `label`, and if the `chunk.label()` is equal to \"NE\". If so, print that chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting practice\n",
    "\n",
    "In this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.\n",
    "\n",
    "You'll use a `defaultdict` called `ner_categories`, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called `chunked_sentences` similar to the last exercise, but this time with non-binary category names.\n",
    "\n",
    "You can use `hasattr()` to determine if each chunk has a `'label'` and then simply use the chunk's `.label()` method as the dictionary key.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Create a defaultdict called ner_categories, with the default type set to int.\n",
    "\n",
    "#### Section 2\n",
    "* Fill up the dictionary with values for each of the keys. Remember, the keys will represent the `label()`.\n",
    "    * In the outer for loop, iterate over `chunked_sentences`, using `sent` as your iterator variable.\n",
    "    * In the inner for loop, iterate over `sent`. If the condition is true, increment the value of each key by 1.\n",
    "    * Remember to use the chunk's `.label()` method as the key!\n",
    "* For the pie chart labels, create a list called `labels` from the keys of `ner_categories`, which can be accessed using `.keys()`.\n",
    "\n",
    "#### Section 3\n",
    "* Use a list comprehension to create a list called `values`, using the `.get()` method on `ner_categories` to compute the values of each label `v`.\n",
    "* Use `plt.pie()` to create a pie chart for each of the NER categories. Along with values and `labels=labels`, pass the extra keyword arguments `autopct='%1.1f%%'` and `startangle=140` to add percentages to the chart and rotate the initial start angle.\n",
    "    * This step has been done for you.\n",
    "* Display your pie chart. Was the distribution what you expected?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing `NLTK` with `spaCy` NER\n",
    "\n",
    "Using the same text you used in the first exercise of this chapter, you'll now see the results using `spaCy`'s NER annotator. How will they compare?\n",
    "\n",
    "The article has been pre-loaded as `article`. To minimize execution times, you'll be asked to specify the keyword arguments `tagger=False`, `parser=False`, `matcher=False` when loading the `spaCy` model, because you only care about the entity in this exercise.\n",
    "\n",
    "### Instructions\n",
    "* Import `spacy`.\n",
    "* Load the `'en'` model using `spacy.load()`. Specify the additional keyword arguments `tagger=False`, `parser=False`, `matcher=False`.\n",
    "* Create a `spacy` document object by passing article into `nlp()`.\n",
    "* Using `ent` as your iterator variable, iterate over the entities of `doc` and print out the labels (`ent.label_`) and text (`ent.text`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French NER with polyglot I\n",
    "\n",
    "In this exercise and the next, you'll use the `polyglot` library to identify French entities. The library functions slightly differently than `spacy`, so you'll use a few of the new things you learned in the last video to display the named entity text and category.\n",
    "\n",
    "You have access to the full article string in `article`. Additionally, the `Text` class of `polyglot` has been imported from `polyglot.text`.\n",
    "\n",
    "### Instructions\n",
    "* Using the article string in `article`, create a new `Text` object called `txt`.\n",
    "* Iterate over `txt.entities` and print each entity, `ent`.\n",
    "* Print the `type()` of `ent`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of ent\n",
    "print(type(ent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French NER with polyglot II\n",
    "\n",
    "Here, you'll complete the work you began in the previous exercise.\n",
    "\n",
    "Your task is to use a list comprehension to create a list of tuples, in which the first element is the entity tag, and the second element is the full string of the entity text.\n",
    "\n",
    "### Instructions\n",
    "* Use a list comprehension to create a list of tuples called `entities`.\n",
    "* The output expression of your list comprehension should be a tuple.\n",
    "    * The first element of each tuple is the entity tag, which you can access using its `.tag` attribute.\n",
    "    * The second element is the full string of the entity text, which you can access using `.join(ent)`.\n",
    "* Your iterator variable should be `ent`, and you should iterate over all of the entities of the `polyglot` `Text` object, `txt`.\n",
    "* Print `entities` to see what you've created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tuples: entities\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print entities\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish NER with polyglot\n",
    "\n",
    "You'll continue your exploration of `polyglot` now with some Spanish annotation. This article is not written by a newspaper, so it is your first example of a more blog-like text. How do you think that might compare when finding entities?\n",
    "\n",
    "The Text object has been created as txt, and each entity has been printed, as you can see in the IPython Shell.\n",
    "\n",
    "Your specific task is to determine how many of the entities contain the words \"Márquez\" or \"Gabo\" - these refer to the same person in different ways!\n",
    "\n",
    "### Instructions\n",
    "* Iterate over all of the entities of `txt`, using `ent` as your iterator variable.\n",
    "* Check whether the entity contains \"Márquez\" or \"Gabo\". If it does, increment count. Don't forget to include the accented á in \"Márquez\"!\n",
    "* Hit 'Submit Answer' to see what percentage of entities refer to Gabriel García Márquez (aka Gabo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the count variable: count\n",
    "count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "for ent in txt.entities:\n",
    "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
    "    if \"Márquez\" in ent or \"Gabo\" in ent:\n",
    "        # Increment count\n",
    "        count += 1\n",
    "\n",
    "# Print count\n",
    "print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "percentage = count / len(txt.entities)\n",
    "print(percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 4 - Building a 'fake news' classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer for text classification\n",
    "\n",
    "It's time to begin building your text classifier! The data has been loaded into a `DataFrame` called `df`. Explore it in the IPython Shell to investigate what columns you can use. The `.head()` method is particularly informative.\n",
    "\n",
    "In this exercise, you'll use pandas alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you'll set up a CountVectorizer and investigate some of its features.\n",
    "\n",
    "### Instructions\n",
    "* Import `CountVectorizer` from `sklearn.feature_extraction.text` and `train_test_split` from `sklearn.model_selection`.\n",
    "* Create a Series `y` to use for the labels by assigning the `.label` attribute of `df` to `y`.\n",
    "* Using `df[\"text\"]` (features) and `y` (labels), create training and test sets using `train_test_split()`. Use a `test_size` of 0.33 and a `random_state` of 53.\n",
    "* Create a `CountVectorizer` object called `count_vectorizer`. Ensure you specify the keyword argument `stop_words=\"english\"` so that stop words are removed.\n",
    "* Fit and transform the training data `X_train` using the `.fit_transform()` method of your `CountVectorizer` object. Do the same with the test data `X_test`, except using the `.transform()` method.\n",
    "* Print the first 10 features of the `count_vectorizer` using its `.get_feature_names()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TfidfVectorizer` for text classification\n",
    "\n",
    "Similar to the sparse `CountVectorizer` created in the previous exercise, you'll work on creating `tf-idf` vectors for your documents. You'll set up a `TfidfVectorizer` and investigate some of its features.\n",
    "\n",
    "In this exercise, you'll use `pandas` and `sklearn` along with the same `X_train`, `y_train` and `X_test`, `y_test` DataFrames and Series you created in the last exercise.\n",
    "\n",
    "### Instructions\n",
    "* Import `TfidfVectorizer` from `sklearn.feature_extraction.text`.\n",
    "* Create a `TfidfVectorizer` object called `tfidf_vectorizer`. When doing so, specify the keyword arguments `stop_words=\"english\"` and `max_df=0.7`.\n",
    "* Fit and transform the training data.\n",
    "* Transform the test data.\n",
    "* Print the first 10 features of `tfidf_vectorizer`.\n",
    "* Print the first 5 vectors of the `tfidf` training data using slicing on the `.A` (or array) attribute of `tfidf_train`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test, y_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the vectors\n",
    "\n",
    "To get a better idea of how the vectors work, you'll investigate them by converting them into `pandas` DataFrames.\n",
    "\n",
    "Here, you'll use the same data structures you created in the previous two exercises (`count_train`, `count_vectorizer`, `tfidf_train`, `tfidf_vectorizer`) as well as `pandas`, which is imported as `pd`.\n",
    "\n",
    "### Instructions\n",
    "* Create the DataFrames `count_df` and `tfidf_df` by using `pd.DataFrame()` and specifying the values as the first argument and the columns (or features) as the second argument.\n",
    "    * The values can be accessed by using the `.A` attribute of, respectively, `count_train` and `tfidf_train`.\n",
    "    * The columns can be accessed using the `.get_feature_names()` methods of `count_vectorizer` and `tfidf_vectorizer`.\n",
    "* Print the head of each DataFrame to investigate their structure. This has been done for you.\n",
    "* Test if the column names are the same for each DataFrame by creating a new object called `difference` to see the difference between the columns that `count_df` has from `tfidf_df`. Columns can be accessed using the `.columns` attribute of a DataFrame. Subtract the set of `tfidf_df.columns` from the set of `count_df.columns`.\n",
    "* Test if the two DataFrames are equivalent by using the `.equals()` method on `count_df` with `tfidf_df` as the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the \"fake news\" model with `CountVectorizer`\n",
    "\n",
    "Now it's your turn to train the \"fake news\" model using the features you identified and extracted. In this first exercise you'll train and test a Naive Bayes model using the `CountVectorizer` data.\n",
    "\n",
    "The training and test sets have been created, and `count_vectorizer`, `count_train`, and `count_test` have been computed.\n",
    "\n",
    "### Instructions\n",
    "* Import the `metrics` module from `sklearn` and `MultinomialNB` from `sklearn.naive_bayes`.\n",
    "* Instantiate a `MultinomialNB` classifier called `nb_classifier`.\n",
    "* Fit the classifier to the training data.\n",
    "* Compute the predicted tags for the test data.\n",
    "* Calculate and print the accuracy score of the classifier.\n",
    "* Compute the confusion matrix. To make it easier to read, specify the keyword argument `labels=['FAKE', 'REAL']`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE','REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the \"fake news\" model with` TfidfVectorizer`\n",
    "\n",
    "Now that you have evaluated the model using the `CountVectorizer`, you'll do the same using the `TfidfVectorizer` with a Naive Bayes model.\n",
    "\n",
    "The training and test sets have been created, and `tfidf_vectorizer`, `tfidf_train`, and `tfidf_test` have been computed. Additionally, `MultinomialNB` and `metrics` have been imported from, respectively, `sklearn.naive_bayes` and `sklearn`.\n",
    "\n",
    "### Instructions\n",
    "* Instantiate a `MultinomialNB` classifier called `nb_classifier`.\n",
    "* Fit the classifier to the training data.\n",
    "* Compute the predicted tags for the test data.\n",
    "* Calculate and print the accuracy score of the classifier.\n",
    "* Compute the confusion matrix. As in the previous exercise, specify the keyword argument `labels=['FAKE', 'REAL']` so that the resulting confusion matrix is easier to read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving your model\n",
    "\n",
    "Your job in this exercise is to test a few different `alpha` levels using the `Tfidf` vectors to determine if there is a better performing combination.\n",
    "\n",
    "The training and test sets have been created, and `tfidf_vectorizer`, `tfidf_train`, and `tfidf_test` have been computed.\n",
    "\n",
    "### Instructions\n",
    "* Create a list of alphas to try using `np.arange()`. Values should range from 0 to 1 with steps of 0.1.\n",
    "* Create a function `train_and_predict()` that takes in one argument: `alpha`. The function should:\n",
    "    * Instantiate a `MultinomialNB` classifier with `alpha=alpha`.\n",
    "    * Fit it to the training data.\n",
    "    * Compute predictions on the test data.\n",
    "    * Compute and return the accuracy score.\n",
    "* Using a for loop, print the `alpha`, `score` and a newline in between. Use your `train_and_predict()` function to compute the score. Does the score change along with the alpha? What is the best alpha?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, 0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting your model\n",
    "\n",
    "Now that you have built a \"fake news\" classifier, you'll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques.\n",
    "\n",
    "You have your well performing `tfidf` Naive Bayes classifier available as `nb_classifier`, and the vectors as `tfidf_vectorizer`.\n",
    "\n",
    "### Instructions\n",
    "* Save the class labels as `class_labels` by accessing the `.classes_` attribute of `nb_classifier`.\n",
    "* Extract the features using the `.get_feature_names()` method of `tfidf_vectorizer`.\n",
    "* Create a zipped array of the classifier coefficients with the feature names and sort them by the coefficients. To do this, first use `zip()` with the arguments `nb_classifier.coef_[0]` and `feature_names`. Then, use `sorted()` on this.\n",
    "* Print the top 20 weighted features for the first label of `class_labels` and print the bottom 20 weighted features for the second label of `class_labels`. This has been done for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n"
   ]
  }
 ]
}