{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Ch. 1 - Univariate Investment Risk and Returns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Financial timeseries data\n",
    "In finance, it is common to be working with a CSV (comma-separated-values) \"flat\" file of a timeseries of many different assets with their prices, returns, or other data over time. Sometimes the data is stored in databases, but more often than not, even large banks still use spreadsheets.\n",
    "\n",
    "In this exercise, you have been given a timeseries of trading data for Microsoft stock as a .csv file stored at the url `fpath_csv`. When you finish the exercise, take note of the various types of data stored in each column.\n",
    "\n",
    "You will be using pandas to read in the CSV data as a DataFrame.\n",
    "\n",
    "### Instructions\n",
    "#### Section 1\n",
    "* Import `pandas` as `pd`.\n",
    "\n",
    "#### Section 2\n",
    "* Use `pd.read_csv()` to read in the file from `fpath_csv` and make sure you parse the `'Date'` column correctly using the `parse_dates` argument.\n",
    "\n",
    "#### Section 3\n",
    "* Ensure the DataFrame is sorted by the `'Date'` column.\n",
    "\n",
    "#### Section 4\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Section 2\n",
    "# Read in the csv file and parse dates\n",
    "StockPrices = pd.read_csv(fpath_csv, parse_dates=['Date'])\n",
    "\n",
    "# Section 3\n",
    "# Ensure the prices are sorted by Date\n",
    "StockPrices = StockPrices.sort_values(by='Date')\n",
    "\n",
    "# Section 4\n",
    "# Print only the first five rows of StockPrices\n",
    "print(StockPrices.head())\n"
   ]
  },
  {
   "source": [
    "## Calculating financial returns\n",
    "The file you loaded in the previous exercise included daily Open, High, Low, Close, Adjusted Close, and Volume data, often referred to as OHLCV data.\n",
    "\n",
    "The Adjusted Close column is the most important. It is normalized for stock splits, dividends, and other corporate actions, and is a true reflection of the return of the stock over time. You will be using the adjusted close price to calculate the returns of the stock in this exercise.\n",
    "\n",
    "`StockPrices` from the previous exercise is available in your workspace, and `matplotlib.pyplot` is imported as `plt`.\n",
    "\n",
    "### Instructions\n",
    "#### Section 1\n",
    "* Calculate the simple returns of the stock on the `'Adjusted'` column and save it to the `'Returns'` column.\n",
    "\n",
    "#### Section 2\n",
    "* Print the first five rows of `StockPrices`.\n",
    "\n",
    "#### Section 3\n",
    "* Use the Pandas `.plot()` method to plot the `'Returns'` column over time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "# Calculate the daily returns of the adjusted close price\n",
    "StockPrices['Returns'] = StockPrices['Adjusted'].pct_change()\n",
    "\n",
    "# Section 2\n",
    "# Check the first five rows of StockPrices\n",
    "print(StockPrices.head())\n",
    "\n",
    "# Section 3\n",
    "# Plot the returns column over time\n",
    "StockPrices['Returns'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Return distributions\n",
    "In order to analyze the probability of outliers in returns, it is helpful to visualize the historical returns of a stock using a histogram.\n",
    "\n",
    "You can use the histogram to show the historical density or frequency of a given range of returns. Note the outliers on the left tail of the return distribution are what you often want to avoid, as they represent large negative daily returns. Outliers on the right hand side of the distribution are normally particularly good events for the stock such as a positive earnings surprise.\n",
    "\n",
    "`StockPrices` from the previous exercise is available in your workspace, and `matplotlib.pyplot` is imported as `plt`.\n",
    "\n",
    "### Instructions\n",
    "#### Section 1\n",
    "* Convert the `'Returns'` column from decimal to percentage returns and assign it to `percent_return`.\n",
    "\n",
    "#### Section 2\n",
    "* Drop the missing values (represented as `NaN`) from `percent_return` and save the result to `returns_plot`.\n",
    "\n",
    "#### Section 3\n",
    "* Plot a histogram of `returns_plot` with 75 bins using `matplotlib`'s `hist()` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "# Convert the decimal returns into percentage returns\n",
    "percent_return = StockPrices['Returns'] * 100\n",
    "\n",
    "# Secion 2\n",
    "# Drop the missing values\n",
    "returns_plot = percent_return.dropna()\n",
    "\n",
    "# Section 3\n",
    "# Plot the returns histogram\n",
    "plt.hist(returns_plot, bins=75)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## First moment: Mu\n",
    "You can calculate the average historical return of a stock by using `numpy`'s `mean()` function.\n",
    "\n",
    "When you are calculating the average daily return of a stock, you are essentially estimating the first moment ($\\mu$) of the historical returns distribution.\n",
    "\n",
    "But what use are daily return estimates to a long-term investor? You can use the formula below to estimate the average annual return of a stock given the average daily return and the number of trading days in a year (typically there are roughly 252 trading days in a year):\n",
    "\n",
    "$$\\text{Average Annualized Return} = (1 + \\mu)^{252} - 1$$\n",
    "\n",
    "The `StockPrices` object from the previous exercise is stored as a variable.\n",
    "\n",
    "### Instructions\n",
    "* Import `numpy` as `np`.\n",
    "* Calculate the mean of the 'Returns' column to estimate the first moment ($\\mu$) and set it equal to `mean_return_daily`.\n",
    "* Use the formula to derive the average annualized return assuming 252 trading days per year. Remember that exponents in Python are calculated using the `**` operator."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the average daily return of the stock\n",
    "mean_return_daily = np.mean(StockPrices['Returns'])\n",
    "print(mean_return_daily)\n",
    "\n",
    "# Calculate the implied annualized average return\n",
    "mean_return_annualized = ((1 + mean_return_daily)**252) - 1\n",
    "print(mean_return_annualized)"
   ]
  },
  {
   "source": [
    "## Second moment: Variance\n",
    "Just like you estimated the first moment of the returns distribution in the last exercise, you can can also estimate the second moment, or variance of a return distribution using numpy.\n",
    "\n",
    "In this case, you will first need to calculate the daily standard deviation ($\\sigma$), or volatility of the returns using `np.std()`. The variance is simply $\\sigma^2$.\n",
    "\n",
    "`StockPrices` from the previous exercise is available in your workspace, and `numpy` is imported as `np`.\n",
    "\n",
    "### Instructions\n",
    "* Calculate the daily standard deviation of the 'Returns' column and set it equal to `sigma_daily`.\n",
    "* Derive the daily variance (second moment, $\\sigma^2$) by squaring the standard deviation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of daily return of the stock\n",
    "sigma_daily = np.std(StockPrices['Returns'])\n",
    "print(sigma_daily)\n",
    "\n",
    "# Calculate the daily variance\n",
    "variance_daily = sigma_daily ** 2\n",
    "print(variance_daily)"
   ]
  },
  {
   "source": [
    "## Annualizing variance\n",
    "You can't annualize the variance in the same way that you annualized the mean.\n",
    "\n",
    "In this case, you will need to multiply $\\sigma$ by the square root of the number of trading days in a year. There are typically 252 trading days in a calendar year. Let's assume this is the case for this exercise.\n",
    "\n",
    "This will get you the annualized volatility, but to get annualized variance, you'll need to square the annualized volatility just like you did for the daily calculation.\n",
    "\n",
    "`sigma_daily` from the previous exercise is available in your workspace, and `numpy` is imported as `np`.\n",
    "\n",
    "### Instructions\n",
    "* Annualize `sigma_daily` by multiplying by the square root of 252 (the number of trading days in a years).\n",
    "* Once again, square `sigma_annualized` to derive the annualized variance."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualize the standard deviation\n",
    "sigma_annualized = sigma_daily * np.sqrt(252)\n",
    "print(sigma_annualized)\n",
    "\n",
    "# Calculate the annualized variance\n",
    "variance_annualized = sigma_annualized ** 2\n",
    "print(variance_annualized)"
   ]
  },
  {
   "source": [
    "## Third moment: Skewness\n",
    "To calculate the third moment, or skewness of a returns distribution in Python, you can use the `skew()` function from `scipy.stats`.\n",
    "\n",
    "Remember that a negative skew is a right-leaning curve, while positive skew is a left-leaning curve. In finance, you would tend to want positive skewness, as this would mean that the probability of large positive returns is unusually high, and the negative returns are more closely clustered and predictable.\n",
    "\n",
    "`StockPrices` from the previous exercise is available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "* Import `skew` from `scipy.stats`.\n",
    "* Drop missing values in the 'Returns' column to prevent errors.\n",
    "* Calculate the skewness of `clean_returns`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import skew from scipy.stats\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Drop the missing values\n",
    "clean_returns = StockPrices['Returns'].dropna()\n",
    "\n",
    "# Calculate the third moment (skewness) of the returns distribution\n",
    "returns_skewness = skew(clean_returns)\n",
    "print(returns_skewness)"
   ]
  },
  {
   "source": [
    "## Fourth moment: Kurtosis\n",
    "Finally, to calculate the fourth moment of a distribution, you can use the `kurtosis()` function from `scipy.stats`.\n",
    "\n",
    "Note that this function actually returns the excess kurtosis, not the 4th moment itself. In order to calculate kurtosis, simply add 3 to the excess kurtosis returned by `kurtosis()`.\n",
    "\n",
    "`clean_returns` from the previous exercise is available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "* Import `kurtosis` from `scipy.stats`.\n",
    "* Use `clean_returns` to calculate the `excess_kurtosis`.\n",
    "* Derive the `fourth_moment` from `excess_kurtosis`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import kurtosis from scipy.stats\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "# Calculate the excess kurtosis of the returns distribution\n",
    "excess_kurtosis = kurtosis(clean_returns)\n",
    "print(excess_kurtosis)\n",
    "\n",
    "# Derive the true fourth moment of the returns distribution\n",
    "fourth_moment = excess_kurtosis + 3\n",
    "print(fourth_moment)"
   ]
  },
  {
   "source": [
    "## Statistical tests for normality\n",
    "In order to truly be confident in your judgement of the normality of the stock's return distribution, you will want to use a true statistical test rather than simply examining the kurtosis or skewness.\n",
    "\n",
    "You can use the `shapiro()` function from `scipy.stats` to run a Shapiro-Wilk test of normality on the stock returns. The function will return two values in a list. The first value is the t-stat of the test, and the second value is the p-value. You can use the p-value to make a judgement about the normality of the data. If the p-value is less than or equal to 0.05, you can safely reject the null hypothesis of normality and assume that the data are non-normally distributed.\n",
    "\n",
    "`clean_returns` from the previous exercise is available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "* Import `shapiro` from `scipy.stats`.\n",
    "* Run the Shapiro-Wilk test on `clean_returns`.\n",
    "* Extract the p-value from the `shapiro_results` tuple."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shapiro from scipy.stats\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Run the Shapiro-Wilk test on the stock returns\n",
    "shapiro_results = shapiro(clean_returns)\n",
    "print(\"Shapiro results:\", shapiro_results)\n",
    "\n",
    "# Extract the p-value from the shapiro_results\n",
    "p_value = shapiro_results[-1]\n",
    "print(\"P-value: \", p_value)"
   ]
  },
  {
   "source": [
    "# Ch. 2 - Portfolio Investing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Calculating portfolio returns\n",
    "In order to build and backtest a portfolio, you have to be comfortable working with the returns of multiple assets in a single object.\n",
    "\n",
    "In this exercise, you will be using a `pandas` DataFrame object, already stored as the variable `StockReturns`, to hold the returns of multiple assets and to calculate the returns of a model portfolio.\n",
    "\n",
    "The model portfolio is constructed with pre-defined weights for some of the largest companies in the world just before January 2017:\n",
    "\n",
    "| Company Name     | Ticker    | Portfolio Weight |\n",
    "| ---              | ---       | ---              |\n",
    "|Apple             |\tAAPL   |\t12%           |\n",
    "|Microsoft         |\tMSFT   |  \t15%           |\n",
    "|Exxon Mobil       | XOM\t   |    8%            |\n",
    "|Johnson & Johnson |\tJNJ    |\t5%            |\n",
    "|JP Morgan         |\tJPM    |\t9%            |\n",
    "|Amazon            |\tAMZN   |\t10%           |\n",
    "|General Electric  |\tGE     |\t11%           |\n",
    "|Facebook          |\tFB     |\t14%           |\n",
    "|AT&T              |\tT      |\t16%           |\n",
    "_Note that the portfolio weights should sum to 100% in most cases_\n",
    "\n",
    "### Instructions\n",
    "* Finish defining the `numpy` array of model `portfolio_weights` with the values according to the table above.\n",
    "* Use the `.mul()` method to multiply the `portfolio_weights` across the rows of `StockReturns` to get weighted stock returns.\n",
    "* Then use the `.sum()` method across the rows on the `WeightedReturns` object to calculate the portfolio returns.\n",
    "* Finally, review the plot of cumulative returns over time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish defining the portfolio weights as a numpy array\n",
    "portfolio_weights = np.array([0.12, 0.15, 0.08, 0.05, 0.09, 0.10, 0.11, 0.14, 0.16])\n",
    "\n",
    "# Calculate the weighted stock returns\n",
    "WeightedReturns = StockReturns.mul(portfolio_weights, axis=1)\n",
    "\n",
    "# Calculate the portfolio returns\n",
    "StockReturns['Portfolio'] = WeightedReturns.sum(axis=1)\n",
    "\n",
    "# Plot the cumulative portfolio returns over time\n",
    "CumulativeReturns = ((1+StockReturns[\"Portfolio\"]).cumprod()-1)\n",
    "CumulativeReturns.plot()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Equal weighted portfolios\n",
    "When comparing different portfolios, you often want to consider performance versus a naive equally-weighted portfolio. If the portfolio doesn't outperform a simple equally weighted portfolio, you might want to consider another strategy, or simply opt for the naive approach if all else fails. You can expect equally-weighted portfolios to tend to outperform the market when the largest companies are doing poorly. This is because even tiny companies would have the same weight in your equally-weighted portfolio as Apple or Amazon, for example.\n",
    "\n",
    "To make it easier for you to visualize the cumulative returns of portfolios, we defined the function `cumulative_returns_plot()` in your workspace.\n",
    "\n",
    "### Instructions\n",
    "* Set `numstocks` equal to 9, which is the number of stocks in your portfolio.\n",
    "* Use `np.repeat()` to set `portfolio_weights_ew` equal to an array with an equal weights for each of the 9 stocks.\n",
    "* Use the `.iloc` accessor to select all rows and the first 9 columns when calculating the portfolio return.\n",
    "* Finally, review the plot of cumulative returns over time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many stocks are in your portfolio?\n",
    "numstocks = 9\n",
    "\n",
    "# Create an array of equal weights across all assets\n",
    "portfolio_weights_ew = np.repeat(1/numstocks, numstocks)\n",
    "\n",
    "# Calculate the equally-weighted portfolio returns\n",
    "StockReturns['Portfolio_EW'] = StockReturns.iloc[:, :9].mul(portfolio_weights_ew, axis=1).sum(axis=1)\n",
    "cumulative_returns_plot(['Portfolio', 'Portfolio_EW'])"
   ]
  },
  {
   "source": [
    "## Market-cap weighted portfolios\n",
    "Conversely, when large companies are doing well, market capitalization, or \"market cap\" weighted portfolios tend to outperform. This is because the largest weights are being assigned to the largest companies, or the companies with the largest market cap.\n",
    "\n",
    "Below is a table of the market capitalizations of the companies in your portfolio just before January 2017:\n",
    "\n",
    "| Company Name      | Ticker | Market Cap ($ Billions) |\n",
    "| ----------------- | ------ | ----------------------- |\n",
    "| Apple             | AAPL   | 601.51                  | \n",
    "| Microsoft         | MSFT\t | 469.25                  |\n",
    "| Exxon Mobil       | XOM\t | 349.5                   |\n",
    "| Johnson & Johnson | JNJ\t | 310.48                  |\n",
    "| JP Morgan\t        | JPM\t | 299.77                  |\n",
    "| Amazon\t        | AMZN   | 356.94                  |\n",
    "| General Electric\t| GE\t | 268.88                  |\n",
    "| Facebook\t        | FB\t | 331.57                  |\n",
    "| AT&T\t            | T\t     | 246.09                  |\n",
    "\n",
    "### Instructions\n",
    "* Finish defining the `market_capitalizations` array of market capitalizations in billions according to the table above.\n",
    "* Calculate `mcap_weights` array such that each element is the ratio of market cap of the company to the total market cap of all companies.\n",
    "* Use the `.mul()` method on the `mcap_weights` and returns to calculate the market capitalization weighted portfolio returns.\n",
    "* Finally, review the plot of cumulative returns over time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Create an array of market capitalizations (in billions)\n",
    "market_capitalizations = np.array([601.51, 469.25, 349.5, 310.48, 299.77, 356.94, 268.88, 331.57, 246.09])\n",
    "\n",
    "# Calculate the market cap weights\n",
    "mcap_weights = market_capitalizations / sum(market_capitalizations)\n",
    "\n",
    "# Calculate the market cap weighted portfolio returns\n",
    "StockReturns['Portfolio_MCap'] = StockReturns.iloc[:, 0:9].mul(mcap_weights, axis=1).sum(axis=1)\n",
    "cumulative_returns_plot(['Portfolio', 'Portfolio_EW', 'Portfolio_MCap'])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## The correlation matrix\n",
    "The correlation matrix can be used to estimate the linear historical relationship between the returns of multiple assets. You can use the built-in `.corr()` method on a pandas DataFrame to easily calculate the correlation matrix.\n",
    "\n",
    "Correlation ranges from -1 to 1. The diagonal of the correlation matrix is always 1, because a stock always has a perfect correlation with itself. The matrix is symmetric, which means that the lower triangle and upper triangle of the matrix are simply reflections of each other since correlation is a bi-directional measurement.\n",
    "\n",
    "In this exercise, you will use the `seaborn` library to generate a heatmap.\n",
    "\n",
    "### Instructions\n",
    "#### Section 1\n",
    "* Calculate the `correlation_matrix` of the `StockReturns` DataFrame.\n",
    "\n",
    "#### Section 2\n",
    "* Import `seaborn` as `sns`.\n",
    "* Use `seaborn`'s `heatmap()` function to create a heatmap to map `correlation_matrix`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = StockReturns.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Section 2\n",
    "# Import seaborn as sns\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(correlation_matrix,\n",
    "            annot=True,\n",
    "            cmap=\"YlGnBu\", \n",
    "            linewidths=0.3,\n",
    "            annot_kws={\"size\": 8})\n",
    "\n",
    "# Plot aesthetics\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0) \n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## The co-variance matrix\n",
    "You can easily compute the co-variance matrix of a DataFrame of returns using the `.cov()` method.\n",
    "\n",
    "The correlation matrix doesn't really tell you anything about the variance of the underlying assets, only the linear relationships between assets. The co-variance (a.k.a. variance-covariance) matrix, on the other hand, contains all of this information, and is very useful for portfolio optimization and risk management purposes.\n",
    "\n",
    "### Instructions\n",
    "* Calculate the co-variance matrix of the `StockReturns` DataFrame.\n",
    "* Annualize the co-variance matrix by multiplying it with 252, the number of trading days in a year."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the covariance matrix\n",
    "cov_mat = StockReturns.cov()\n",
    "\n",
    "# Annualize the co-variance matrix\n",
    "cov_mat_annual = cov_mat * 252\n",
    "\n",
    "# Print the annualized co-variance matrix\n",
    "print(cov_mat_annual)"
   ]
  },
  {
   "source": [
    "## Portfolio standard deviation\n",
    "In order to calculate portfolio volatility, you will need the covariance matrix, the portfolio weights, and knowledge of the transpose operation. The transpose of a numpy array can be calculated using the .T attribute. The np.dot() function is the dot-product of two arrays.\n",
    "\n",
    "The formula for portfolio volatility is:\n",
    "\n",
    "$$ \\sigma_{\\text{Portfolio}} = \\sqrt{w_T \\times \\Sigma \\times w} $$\n",
    "\n",
    "* $\\sigma_{\\text{Portfolio}}$: Portfolio volatility\n",
    "* $\\Sigma$: Covariance matrix of returns\n",
    "* $w$: Portfolio weights ($w_T$ is transposed portfolio weights)\n",
    "* $\\cdot$: The dot-multiplication operator `portfolio_weights` and `cov_mat_annual` are available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "* Calculate the portfolio volatility assuming you use the `portfolio_weights` by following the formula above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the portfolio standard deviation\n",
    "portfolio_volatility = np.sqrt(np.dot(portfolio_weights.T, np.dot(cov_mat_annual, portfolio_weights)))\n",
    "print(portfolio_volatility)"
   ]
  },
  {
   "source": [
    "## Sharpe ratios\n",
    "The Sharpe ratio is a simple metric of risk adjusted return which was pioneered by William F. Sharpe. Sharpe ratio is useful to determine how much risk is being taken to achieve a certain level of return. In finance, you are always seeking ways to improve your Sharpe ratio, and the measure is very commonly quoted and used to compare investment strategies.\n",
    "\n",
    "The original 1966 Sharpe ratio calculation is quite simple:\n",
    "\n",
    "$$ S = \\frac{R_a - r_f}{\\sigma_a} $$\n",
    "\n",
    "* $S$: Sharpe Ratio\n",
    "* $R_a$: Asset return\n",
    "* $r_f$: Risk-free rate of return\n",
    "* $\\sigma_a$: Asset volatility\n",
    "* The randomly generated portfolio is available as `RandomPortfolios`.\n",
    "\n",
    "### Instructions\n",
    "* Assume a `risk_free` rate of 0 for this exercise.\n",
    "* Calculate the Sharpe ratio for each asset by subtracting the risk free rate from returns and then dividing by volatility."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk free rate\n",
    "risk_free = 0\n",
    "\n",
    "# Calculate the Sharpe Ratio for each asset\n",
    "RandomPortfolios['Sharpe'] = (RandomPortfolios['Returns'] - risk_free) / RandomPortfolios['Volatility']\n",
    "\n",
    "# Print the range of Sharpe ratios\n",
    "print(RandomPortfolios['Sharpe'].describe()[['min', 'max']])"
   ]
  },
  {
   "source": [
    "## The MSR portfolio\n",
    "The maximum Sharpe ratio, or MSR portfolio, which lies at the apex of the efficient frontier, can be constructed by looking for the portfolio with the highest Sharpe ratio.\n",
    "\n",
    "Unfortunately, the MSR portfolio is often quite erratic. Even though the portfolio had a high historical Sharpe ratio, it doesn't guarantee that the portfolio will have a good Sharpe ratio moving forward.\n",
    "\n",
    "### Instructions\n",
    "* Sort `RandomPortfolios` with the highest Sharpe value, ranking in descending order.\n",
    "* Multiply `MSR_weights_array` across the rows of `StockReturns` to get weighted stock returns.\n",
    "* Finally, review the plot of cumulative returns over time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the portfolios by Sharpe ratio\n",
    "sorted_portfolios = RandomPortfolios.sort_values(by=['Sharpe'], ascending=False)\n",
    "\n",
    "# Extract the corresponding weights\n",
    "MSR_weights = sorted_portfolios.iloc[0, 0:numstocks]\n",
    "\n",
    "# Cast the MSR weights as a numpy array\n",
    "MSR_weights_array = np.array(MSR_weights)\n",
    "\n",
    "# Calculate the MSR portfolio returns\n",
    "StockReturns['Portfolio_MSR'] = StockReturns.iloc[:, 0:numstocks].mul(MSR_weights_array, axis=1).sum(axis=1)\n",
    "\n",
    "# Plot the cumulative returns\n",
    "cumulative_returns_plot(['Portfolio_EW', 'Portfolio_MCap', 'Portfolio_MSR'])"
   ]
  },
  {
   "source": [
    "## The GMV portfolio\n",
    "The global minimum volatility portfolio, or GMV portfolio, is the portfolio with the lowest standard deviation (risk) and the highest return for the given risk level.\n",
    "\n",
    "Returns are very hard to predict, but volatilities and correlations tend to be more stable over time. This means that the GMV portfolio often outperforms the MSR portfolios out of sample even though the MSR would outperform quite significantly in-sample. Of course, out of sample results are what really matters in finance.\n",
    "\n",
    "### Instructions\n",
    "* Sort `RandomPortfolios` with the lowest volatility value, ranking in ascending order.\n",
    "* Multiply `GMV_weights_array` across the rows of StockReturns to get weighted stock returns.\n",
    "* Finally, review the plot of cumulative returns over time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the portfolios by volatility\n",
    "sorted_portfolios = RandomPortfolios.sort_values(by=['Volatility'], ascending=True)\n",
    "\n",
    "# Extract the corresponding weights\n",
    "GMV_weights = sorted_portfolios.iloc[0, 0:numstocks]\n",
    "\n",
    "# Cast the GMV weights as a numpy array\n",
    "GMV_weights_array = np.array(GMV_weights)\n",
    "\n",
    "# Calculate the GMV portfolio returns\n",
    "StockReturns['Portfolio_GMV'] = StockReturns.iloc[:, 0:numstocks].mul(GMV_weights_array, axis=1).sum(axis=1)\n",
    "\n",
    "# Plot the cumulative returns\n",
    "cumulative_returns_plot(['Portfolio_EW', 'Portfolio_MCap', 'Portfolio_MSR', 'Portfolio_GMV'])"
   ]
  },
  {
   "source": [
    "# Ch. 3 - Factor Investing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Excess returns\n",
    "In order to perform a robust analysis on your portfolio returns, you must first subtract the risk-free rate of return from your portfolio returns. The portfolio return minus the risk-free rate of return is known as the Excess Portfolio Return.\n",
    "\n",
    "In the United States, the risk-free rate has been close to 0 since the financial crisis (2008), but this step is crucial for other countries with higher risk-free rates such as Venezuela or Brazil.\n",
    "\n",
    "The FamaFrenchData DataFrame is available in your workspace and contains the proper data for this exercise. The portfolio you will be working with is the equal-weighted portfolio from Chapter 2.\n",
    "\n",
    "### Instructions\n",
    "* Calculate excess portfolio returns by subtracting the risk free (`'RF'`) column from the `'Portfolio'` column in `FamaFrenchData`.\n",
    "* Review the plot of returns and excessive returns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate excess portfolio returns\n",
    "FamaFrenchData['Portfolio_Excess'] = FamaFrenchData['Portfolio'] - FamaFrenchData['RF']\n",
    "\n",
    "# Plot returns vs excess returns\n",
    "CumulativeReturns = ((1+FamaFrenchData[['Portfolio','Portfolio_Excess']]).cumprod()-1)\n",
    "CumulativeReturns.plot()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Calculating beta using co-variance\n",
    "Beta is an essential component of many financial models, and is a measure of systematic risk, or exposure to the broad market. In the CAPM model, beta is one of two essential factors.\n",
    "\n",
    "Historical beta can be estimated in a number of ways. In this exercise, you will use the following simple formula involving co-variance and variance to a benchmark market portfolio:\n",
    "\n",
    "$$ \\beta_P = \\frac{Cov(R_P, R_B)}{Var(R_B)} $$\n",
    "\n",
    "* $\\beta_P$: Portfolio beta\n",
    "* $Cov(R_P, R_B)$: The co-variance between the portfolio (P) and the benchmark market index (B)\n",
    "* $Var(R_B$: The variance of the benchmark market index\n",
    "\n",
    "The `FamaFrenchData` DataFrame is available in your workspace and contains the proper data for this exercise.\n",
    "\n",
    "### Instructions\n",
    "#### Section 1\n",
    "* Generate a co-variance matrix between `'Portfolio_Excess'` and `'Market_Excess'` columns.\n",
    "\n",
    "#### Section 2\n",
    "* Calculate the variance of `'Market_Excess'` column.\n",
    "\n",
    "#### Section 3\n",
    "* Calculate the portfolio beta."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "# Calculate the co-variance matrix between Portfolio_Excess and Market_Excess\n",
    "covariance_matrix = FamaFrenchData[['Portfolio_Excess', 'Market_Excess']].cov()\n",
    "\n",
    "# Extract the co-variance co-efficient\n",
    "covariance_coefficient = covariance_matrix.iloc[0, 1]\n",
    "print(covariance_coefficient)\n",
    "\n",
    "# Section 2\n",
    "# Calculate the benchmark variance\n",
    "benchmark_variance = FamaFrenchData['Market_Excess'].var()\n",
    "print(benchmark_variance)\n",
    "\n",
    "# Section 3\n",
    "# Calculating the portfolio market beta\n",
    "portfolio_beta = covariance_coefficient / benchmark_variance\n",
    "print(portfolio_beta)"
   ]
  },
  {
   "source": [
    "## Calculating beta with CAPM\n",
    "There are many ways to model stock returns, but the Capital Asset Pricing Model, or CAPM, is one the most well known:\n",
    "\n",
    "$$ E(R_P) - r_f = \\beta_P (E(R_M) - r_f) $$\n",
    "\n",
    "* $E(R_P) - r_f$: The excess expected return of a stock or portfolio $P$\n",
    "* $E(R_B) - r_f$: The excess expected return of the broad market portfolio $B$\n",
    "* $r_f$: The regional risk free-rate\n",
    "* $\\beta_P$: Portfolio beta, or exposure, to the broad market portfolio $B$\n",
    "\n",
    "You can call the `.fit()` method from `statsmodels.formula.api` on an `.ols(formula, data)` model object to perform the analysis, and the `.summary()` method on the analysis object to anaylze the results.\n",
    "\n",
    "The `FamaFrenchData` DataFrame is available in your workspace and contains the proper data for this exercise.\n",
    "\n",
    "### Instructions\n",
    "* First, you will need to import `statsmodels.formula.api` as `smf`.\n",
    "* Define a regression model that explains `Portfolio_Excess` as a function of `Market_Excess`.\n",
    "* Extract and print the adjusted r-squared of the fitted regression model.\n",
    "* Extract the market beta of your portfolio."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels.formula.api\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "# Define the regression formula\n",
    "CAPM_model = smf.ols(formula='Portfolio_Excess ~ Market_Excess', data=FamaFrenchData)\n",
    "\n",
    "# Print adjusted r-squared of the fitted regression\n",
    "CAPM_fit = CAPM_model.fit()\n",
    "print(CAPM_fit.rsquared_adj)\n",
    "\n",
    "# Extract the beta\n",
    "regression_beta = CAPM_fit.params['Market_Excess']\n",
    "print(regression_beta)"
   ]
  },
  {
   "source": [
    "## The Fama French 3-factor model\n",
    "The Fama-French model famously adds two additional factors to the CAPM model to describe asset returns:\n",
    "\n",
    "$$ R_{P} = RF + \\beta_{M}(R_{M}-RF)+b_{SMB} \\cdot SMB + b_{HML} \\cdot HML + \\alpha $$\n",
    "\n",
    "* $SMB$: The small minus big factor\n",
    "* $b_{SMB}$: Exposure to the SMB factor\n",
    "* $HML$: The high minus low factor\n",
    "* $b_{HML}$: Exposure to the HML factor\n",
    "* $\\alpha$: Performance which is unexplained by any other factors\n",
    "* $\\beta_M$: Beta to the broad market portfolio B\n",
    "\n",
    "The `FamaFrenchData` DataFrame is available in your workspace and contains the HML and SMB factors as columns for this exercise.\n",
    "\n",
    "### Instructions\n",
    "* Define a regression model that explains Portfolio_Excess as a function of Market_Excess, SMB, and HML.\n",
    "* Extract the adjusted r-squared value from FamaFrench_fit."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels.formula.api\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "# Define the regression formula\n",
    "FamaFrench_model = smf.ols(formula='Portfolio_Excess ~ Market_Excess + SMB + HML', data=FamaFrenchData)\n",
    "\n",
    "# Fit the regression\n",
    "FamaFrench_fit = FamaFrench_model.fit()\n",
    "\n",
    "# Extract the adjusted r-squared\n",
    "regression_adj_rsq = FamaFrench_fit.rsquared_adj\n",
    "print(regression_adj_rsq)"
   ]
  },
  {
   "source": [
    "## p-values and coefficients\n",
    "You can use the `.pvalues` attribute on a fitted `smf.ols` regression model to retrieve the p-values for each coefficient.\n",
    "\n",
    "Normally, p-values less than 0.05 are considered statistically significant.\n",
    "\n",
    "Coefficients can be extracted from the fitted regression object using the `.params` attribute.\n",
    "\n",
    "In this example, a statistically significant negative SMB ('Small Minus Big') coefficient would signify a factor exposure to large cap stocks, while a positive coefficient would signify an exposure to small cap stocks.\n",
    "\n",
    "The fitted regression model `FamaFrench_fit` from the previous exercise is available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "* Extract the p-value for `'SMB'`.\n",
    "* Extract the regression coefficient for `'SMB'`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the p-value of the SMB factor\n",
    "smb_pval = FamaFrench_fit.pvalues['SMB']\n",
    "\n",
    "# If the p-value is significant, print significant\n",
    "if smb_pval < 0.05:\n",
    "    significant_msg = 'significant'\n",
    "else:\n",
    "    significant_msg = 'not significant'\n",
    "\n",
    "# Print the SMB coefficient\n",
    "smb_coeff = FamaFrench_fit.params['SMB']\n",
    "print(\"The SMB coefficient is \", smb_coeff, \" and is \", significant_msg)"
   ]
  },
  {
   "source": [
    "## The efficient market and alpha\n",
    "The alpha ($\\alpha$) left over by the regression is unexplained performance due to unknown factors. In a regression model, this is simply the coefficient of the intercept.\n",
    "\n",
    "There are two general schools of thought as to why:\n",
    "\n",
    "1. The model simply needs to be expanded. When you have found all of the missing economic factors, you can explain all stock and portfolio returns. This is known as the Efficient Market Hypothesis.\n",
    "\n",
    "2. There is a degree of unexplainable performance that no model will ever capture reliably. Perhaps it is due to skill, timing, intuition or luck, but investors should seek to maximize their alpha.\n",
    "\n",
    "Your fitted regression analysis from the previous exercise has been stored in `FamaFrench_fit`.\n",
    "\n",
    "### Instructions\n",
    "* Extract the coefficient of your intercept and assign it to `portfolio_alpha`.\n",
    "* Annualize your `portfolio_alpha` return by assuming 252 trading days in a year."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate your portfolio alpha\n",
    "portfolio_alpha = FamaFrench_fit.params['Intercept']\n",
    "print(portfolio_alpha)\n",
    "\n",
    "# Annualize your portfolio alpha\n",
    "portfolio_alpha_annualized = (1 + portfolio_alpha)**252 - 1\n",
    "print(portfolio_alpha_annualized)"
   ]
  },
  {
   "source": [
    "## The 5-factor model\n",
    "In 2015, Fama and French extended their previous 3-factor model, adding two additional factors:\n",
    "\n",
    "* RMW: Profitability\n",
    "* CMA: Investment\n",
    "\n",
    "The RMW factor represents the returns of companies with high operating profitability versus those with low operating profitability, and the CMA factor represents the returns of companies with aggressive investments versus those who are more conservative.\n",
    "\n",
    "The `FamaFrenchData` object is available in your workspace and contains the RMW and CMA factors in addition to the previous factors.\n",
    "\n",
    "### Instructions\n",
    "* Use what you've learned from the previous exercises to define the `FamaFrench5_model` regression model for `Portfolio_Excess` against the original 3 Fama-French factors (`Market_Excess`, `SMB`, `HML`) in addition to the two new factors (`RMW`, `CMA`).\n",
    "* Fit the regression model and store the results in `FamaFrench5_fit`.\n",
    "* Extract the adjusted r-squared value and assign it to `regression_adj_rsq`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels.formula.api\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "# Define the regression formula\n",
    "FamaFrench5_model = smf.ols(formula='Portfolio_Excess ~ Market_Excess + SMB + HML + RMW + CMA ', data=FamaFrenchData)\n",
    "\n",
    "# Fit the regression\n",
    "FamaFrench5_fit = FamaFrench5_model.fit()\n",
    "\n",
    "# Extract the adjusted r-squared\n",
    "regression_adj_rsq = FamaFrench5_fit.rsquared_adj\n",
    "print(regression_adj_rsq)"
   ]
  },
  {
   "source": [
    "# Ch. 4 - Value at Risk"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Historical drawdown\n",
    "The stock market tends to rise over time, but that doesn't mean that you won't have periods of drawdown.\n",
    "\n",
    "Drawdown can be measured as the percentage loss from the highest cumulative historical point.\n",
    "\n",
    "In Python, you can use the `.accumulate()` and `.maximum()` functions to calculate the running maximum, and the simple formula below to calculate drawdown:\n",
    "\n",
    "$$ \\text{Drawdown} = \\frac{r_t}{\\text{RM}} - 1 $$\n",
    "\n",
    "* $r_t$: Cumulative return at time $t$\n",
    "* $\\text{RM}$: Running maximum\n",
    "\n",
    "The cumulative returns of USO, an ETF that tracks oil prices, is available in the variable `cum_rets`.\n",
    "\n",
    "### Instructions\n",
    "* Calculate the running maximum of the cumulative returns of the USO oil ETF (`cum_rets`) using `np.maximum.accumulate()`.\n",
    "* Where the running maximum (`running_max`) drops below 1, set the running maximum equal to 1.\n",
    "* Calculate drawdown using the simple formula above with the `cum_rets` and `running_max`.\n",
    "* Review the plot."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the running maximum\n",
    "running_max = np.maximum.accumulate(cum_rets)\n",
    "\n",
    "# Ensure the value never drops below 1\n",
    "running_max[running_max < 1] = 1\n",
    "\n",
    "# Calculate the percentage drawdown\n",
    "drawdown = (cum_rets)/running_max - 1\n",
    "\n",
    "# Plot the results\n",
    "drawdown.plot()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Historical value at risk\n",
    "Drawdown is a measure of sustained losses over time, but what about simple single-day movements?\n",
    "\n",
    "Value at Risk, often referred to as VaR, is a way to estimate the risk of a single day negative price movement. VaR can be measured for any given probability, or confidence level, but the most commonly quoted tend to be VaR(95) and VaR(99). Historical VaR is the simplest method to calculate VaR, but relies on historical returns data which may not be a good assumption of the future. Historical VaR(95), for example, represents the minimum loss that your portfolio or asset has sustained in the worst 5% of cases.\n",
    "\n",
    "Below, you will calculate the historical VaR(95) of the USO oil ETF. Returns data is available (in percent) in the variable `StockReturns_perc`.\n",
    "\n",
    "Instructions\n",
    "* Calculate VaR(95), the worst 5% of USO returns (`StockReturns_perc`), and assign it to `var_95`.\n",
    "* Sort `StockReturns_perc` and assign it to `sorted_rets`.\n",
    "* Plot the histogram of sorted returns (sorted_rets)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate historical VaR(95)\n",
    "var_95 = np.percentile(StockReturns_perc, 5)\n",
    "print(var_95)\n",
    "\n",
    "# Sort the returns for plotting\n",
    "sorted_rets = StockReturns_perc.sort_values()\n",
    "\n",
    "# Plot the probability of each sorted return quantile\n",
    "plt.hist(sorted_rets, normed=True)\n",
    "\n",
    "# Denote the VaR 95 quantile\n",
    "plt.axvline(x=var_95, color='r', linestyle='-', label=\"VaR 95: {0:.2f}%\".format(var_95))\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Historical expected shortfall\n",
    "Expected Shortfall, otherwise known as CVaR, or conditional value at risk, is simply the expected loss of the worst case scenarios of returns.\n",
    "\n",
    "For example, if your portfolio has a VaR(95) of -3%, then the CVaR(95) would be the average value of all losses exceeding -3%.\n",
    "\n",
    "Returns data is available (in percent) in the variable `StockReturns_perc`. `var_95` from the previous exercise is also available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "* Calculate the average of returns in `StockReturns_perc` where `StockReturns_perc` is less than or equal to `var_95` and assign it to `cvar_95`.\n",
    "* Plot the histogram of sorted returns (`sorted_rets`) using the `plt.hist()` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical CVaR 95\n",
    "cvar_95 = StockReturns_perc[StockReturns_perc <= var_95].mean()\n",
    "print(cvar_95)\n",
    "\n",
    "# Sort the returns for plotting\n",
    "sorted_rets = sorted(StockReturns_perc)\n",
    "\n",
    "# Plot the probability of each return quantile\n",
    "plt.hist(sorted_rets, normed=True)\n",
    "\n",
    "# Denote the VaR 95 and CVaR 95 quantiles\n",
    "plt.axvline(x=var_95, color=\"r\", linestyle=\"-\", label='VaR 95: {0:.2f}%'.format(var_95))\n",
    "plt.axvline(x=cvar_95, color='b', linestyle='-', label='CVaR 95: {0:.2f}%'.format(cvar_95))\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Changing VaR and CVaR quantiles\n",
    "VaR quantiles often used are 90%, 95%, and 99%, corresponding to the worst 10%, 5%, and 1% of cases respectively. These same quantiles are also often used for CVaR. Note that CVaR will always be a more extreme estimate when compared with VaR for the same quantile.\n",
    "\n",
    "Compare the VaR vs CVaR values for USO ETF returns below.\n",
    "\n",
    "Returns data is available (in percent) in `StockReturns_perc`. We also calculated `var_95`, `cvar_95`, `var_99`, `cvar_99` and defined a function `plot_hist()` that compares several quantiles for you.\n",
    "\n",
    "### Instructions\n",
    "* Calculate the VaR(90) for `StockReturns_perc` and save the result in `var_90`.\n",
    "* Calculate the CVaR(90) for `StockReturns_perc` and save the result in `cvar_90`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical VaR(90) quantiles\n",
    "var_90 = np.percentile(StockReturns_perc, 10)\n",
    "print(var_90)\n",
    "\n",
    "# Historical CVaR(90) quantiles\n",
    "cvar_90 = StockReturns_perc[StockReturns_perc <= var_90].mean()\n",
    "print(cvar_90)\n",
    "\n",
    "# Plot to compare\n",
    "plot_hist()"
   ]
  },
  {
   "source": [
    "## Parametric VaR\n",
    "Value at Risk can also be computed parametrically using a method known as variance/co-variance VaR. This method allows you to simulate a range of possibilities based on historical return distribution properties rather than actual return values. You can calculate the parametric VaR(90) using:\n",
    "\n",
    "```\n",
    "# Import norm from scipy.stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Calculate Parametric VaR\n",
    "norm.ppf(confidence_level=0.10, mu, vol)\n",
    "```\n",
    "where `mu` and `vol` are the mean and volatility, respectively.\n",
    "\n",
    "Returns data is available (in decimals) in the variable `StockReturns`.\n",
    "\n",
    "### Instructions\n",
    "* Import `norm` from `scipy.stats`.\n",
    "* Calculate the mean and volatility of `StockReturns` and assign them to `mu` and `vol`, respectively.\n",
    "* Set the `confidence_level` for VaR(95).\n",
    "* Calculate VaR(95) using the `norm.ppf()` function, passing in the confidence level as the first parameter, with `mu` and `vol` as the second and third parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import norm from scipy.stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Estimate the average daily return\n",
    "mu = np.mean(StockReturns)\n",
    "\n",
    "# Estimate the daily volatility\n",
    "vol = np.std(StockReturns)\n",
    "\n",
    "# Set the VaR confidence level\n",
    "confidence_level = 0.05\n",
    "\n",
    "# Calculate Parametric VaR\n",
    "var_95 = norm.ppf(confidence_level, mu, vol)\n",
    "print('Mean: ', str(mu), '\\nVolatility: ', str(vol), '\\nVaR(95): ', str(var_95))"
   ]
  },
  {
   "source": [
    "## Scaling risk estimates\n",
    "The VaR(95) number calculated in previous exercises is simply the value at risk for a single day. To estimate the VaR for a longer time horizon, scale the value by the square root of time, similar to scaling volatility:\n",
    "\n",
    "\n",
    "`StockReturns_perc` and `var_95` from the previous exercise is available in your workspace. Use this data to estimate the VaR for the USO oil ETF for 1 to 100 days from now. We've also defined a function `plot_var_scale()` that plots the VaR for 1 to 100 days from now.\n",
    "\n",
    "### Instructions\n",
    "* Loop from 0 to 100 (not including 100) using the `range()` function.\n",
    "* Set the second column of `forecasted_values` at each index equal to the forecasted VaR, multiplying `var_95` by the square root of i + 1 using the `np.sqrt()` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate forecasted VaR\n",
    "forecasted_values = np.empty([100, 2])\n",
    "\n",
    "# Loop through each forecast period\n",
    "for i in range(100):\n",
    "    # Save the time horizon i\n",
    "    forecasted_values[i, 0] = i\n",
    "    # Save the forecasted VaR 95\n",
    "    forecasted_values[i, 1] = var_95 * np.sqrt(i + 1)\n",
    "    \n",
    "# Plot the results\n",
    "plot_var_scale()"
   ]
  },
  {
   "source": [
    "## A random walk simulation\n",
    "Stochastic or random movements are used in physics to represent particle and fluid movements, in mathematics to describe fractal behavior, and in finance to describe stock market movements.\n",
    "\n",
    "Use the `np.random.normal()` function to model random walk movements of the USO oil ETF with a constant daily average return (`mu`) and average daily volatility (`vol`) over the course of $T$ trading days.\n",
    "\n",
    "### Instructions\n",
    "* Set the number of simulated days ($T$) equal to 252, and the initial stock price ($S_0$) equal to 10.\n",
    "* Calculate $T$ random normal values using `np.random.normal()`, passing in `mu` and `vol`, and $T$ as parameters, then adding 1 to the values and assign it to `rand_rets`.\n",
    "* Calculate the random walk by multiplying `rand_rets.cumprod()` by the initial stock price and assign it to `forecasted_values`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the simulation parameters\n",
    "mu = np.mean(StockReturns)\n",
    "vol = np.std(StockReturns)\n",
    "T = 252\n",
    "S0 = 10\n",
    "\n",
    "# Add one to the random returns\n",
    "rand_rets = np.random.normal(mu, vol, T) + 1\n",
    "\n",
    "# Forecasted random walk\n",
    "forecasted_values = rand_rets.cumprod() * S0\n",
    "\n",
    "# Plot the random walk\n",
    "plt.plot(range(0, T), forecasted_values)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Monte Carlo simulations\n",
    "Monte-Carlo simulations are used to model a wide range of possibilities.\n",
    "\n",
    "Monte-Carlos can be constructed in many different ways, but all of them involve generating a large number of random variants of a given model, allowing a wide distribution of possible paths to be analyzed. This can allow you to build a comprehensive forecast of possibilities to sample from without a large amount of historical data.\n",
    "\n",
    "Generate 100 Monte-Carlo simulations for the USO oil ETF.\n",
    "\n",
    "The parameters `mu`, `vol`, `T`, and `S0` are available from the previous exercise.\n",
    "\n",
    "### Instructions\n",
    "* Loop from 0 to 100 (not including 100) using the `range()` function.\n",
    "* Call the plotting function for each iteration using the `plt.plot()` function, passing the range of values $T$ (`range(T)`) as the first argument and the `forecasted_values` as the second argument."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through 100 simulations\n",
    "for i in range(100):\n",
    "\n",
    "    # Generate the random returns\n",
    "    rand_rets = np.random.normal(mu, vol, T) + 1\n",
    "    \n",
    "    # Create the Monte carlo path\n",
    "    forecasted_values = S0*(rand_rets).cumprod()\n",
    "    \n",
    "    # Plot the Monte Carlo path\n",
    "    plt.plot(range(T), forecasted_values)\n",
    "\n",
    "# Show the simulations\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Monte Carlo VaR\n",
    "Both the return values and the Monte-Carlo paths can be used for analysis of everything ranging from option pricing models and hedging to portfolio optimization and trading strategies.\n",
    "\n",
    "Aggregate the returns data at each iteration, and use the resulting values to forecast parametric VaR(99).\n",
    "\n",
    "The parameters `mu`, `vol`, `T`, and `S0` are available from the previous exercise.\n",
    "\n",
    "### Instructions\n",
    "* Use the `.append()` method to append the `rand_rets` to `sim_returns` list in each iteration.\n",
    "* Calculate the parametric VaR(99) using the `np.percentile()` function on `sim_returns`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the returns\n",
    "sim_returns = []\n",
    "\n",
    "# Loop through 100 simulations\n",
    "for i in range(100):\n",
    "\n",
    "    # Generate the Random Walk\n",
    "    rand_rets = np.random.normal(mu, vol, T)\n",
    "    \n",
    "    # Save the results\n",
    "    sim_returns.append(rand_rets)\n",
    "\n",
    "# Calculate the VaR(99)\n",
    "var_99 = np.percentile(sim_returns, 1)\n",
    "print(\"Parametric VaR(99): \", round(100*var_99, 2),\"%\")"
   ]
  }
 ]
}