{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 1 - Sentiment Analysis Nuts and Bolts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many positive and negative reviews are there?\n",
    "\n",
    "As a first step in a sentiment analysis task, similar to other data science problems, we might want to explore the dataset in more detail.\n",
    "\n",
    "You will work with a sample of the IMDB movies reviews. A dataset called `movies` has been created for you. It is a sample of the data we saw in the slides. Feel free to explore it in the IPython Shell, calling the `.head()` method, for example.\n",
    "\n",
    "Be aware that this exercise uses real data, and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real data).\n",
    "\n",
    "### Instructions\n",
    "* Find the number of positive and negative reviews in the `movies` dataset.\n",
    "* Find the proportion (percentage) of positive and negative reviews in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of positive and negative reviews\n",
    "print('Number of positive and negative reviews: ', movies.label.value_counts())\n",
    "\n",
    "# Find the proportion of positive and negative reviews\n",
    "print('Proportion of positive and negative reviews: ', movies.label.value_counts() / len(movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longest and shortest reviews\n",
    "\n",
    "In this exercise, you will continue to work with the `movies` dataset. You explored how many positive and negative reviews there are. Now your task is to explore the review column in more detail.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Use the `review` column of the `movies` dataset to find the length of the longest review.\n",
    "\n",
    "#### Section 2\n",
    "* Similarly, find the length of the shortest review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "length_reviews = movies.review.str.len()\n",
    "\n",
    "# How long is the longest review\n",
    "print(max(length_reviews))\n",
    "\n",
    "# Section 2\n",
    "length_reviews = movies.review.str.len()\n",
    "\n",
    "# How long is the shortest review\n",
    "print(min(length_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting the sentiment of Tale of Two Cities\n",
    "\n",
    "In the video we saw that one type of algorithms for detecting the sentiment are based on a lexicon of predefined words and their corresponding polarity score. Your task in this exercise is to detect the sentiment, including polarity and subjectivity of a given string using such a rule-based approach and the textblob library in Python.\n",
    "\n",
    "You will work with the `two_cities` string. It contains the first sentence of Dickens's Tale of Two Cities novel. Feel free to explore it in the Shell.\n",
    "\n",
    "### Instructions\n",
    "* Create a text blob object from the `two_cities` string.\n",
    "* Print out the polarity and subjectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a textblob object  \n",
    "blob_two_cities = TextBlob(two_cities)\n",
    "\n",
    "# Print out the sentiment \n",
    "print(blob_two_cities.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the sentiment of two strings\n",
    "\n",
    "In this exercise, you will compare the sentiment of two different strings. A string called `annak` has been defined for you and it contains the first sentence of Anna Karenina. A second string called `catcher` has been created and it contains the first sentence of The Catcher in the Rye. Feel free to explore both in the IPython Shell.\n",
    "\n",
    "Your task is again to detect the sentiment of each string - both their polarity and subjectivity. Which one has higher sentiment score? Did you expect that to be the case?\n",
    "\n",
    "### Instructions\n",
    "* Import the required function from the appropriate package.\n",
    "* Create a text blob object from the `annak` string.\n",
    "* Create a text blob from the catcher `string` as well.\n",
    "* Print out the polarity and subjectivity of each of the created blobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a textblob object \n",
    "blob_annak = TextBlob(annak)\n",
    "blob_catcher = TextBlob(catcher)\n",
    "\n",
    "# Print out the sentiment   \n",
    "print('Sentiment of annak: ', blob_annak.sentiment)\n",
    "print('Sentiment of catcher: ', blob_catcher.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the sentiment of a movie review?\n",
    "\n",
    "In a previous exercise, you detected the sentiment of the first sentence of the _Tale of Two Cities_ novel by Dickens. Now you will continue to work with the movie reviews dataset. Do you remember how you found the longest and shortest reviews? One of the longest reviews has been imported for you. It is called `titanic` as it discusses the Titanic movie. Feel free to explore it in the Shell.\n",
    "\n",
    "Can you calculate the polarity and subjectivity of the `titanic` string? This review is positive (i.e. has a label of 1). Is the polarity score also positive?\n",
    "\n",
    "### Instructions\n",
    "* Import the required functionality.\n",
    "* Create a text blob object from the `titanic` string.\n",
    "* Print out the result of its `sentiment` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a textblob object  \n",
    "blob_titanic = TextBlob(titanic)\n",
    "\n",
    "# Print out its sentiment  \n",
    "print(blob_titanic.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your first word cloud\n",
    "\n",
    "We saw in the video that word clouds are very intuitive and a great and fast way to get a first impression on what a piece of text is talking about.\n",
    "\n",
    "In this exercise, you will build your first word cloud. A string `east_of_eden` has been defined for you. It contains one of the first sentences of John Steinbeck's novel _East of Eden_. You can inspect its contents in the IPython Shell.\n",
    "\n",
    "The `matplotlib.pyplot` package has been imported for you as `plt`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Import the required package to build a word cloud.\n",
    "* Generate a word cloud using the `east_of_eden` string. The background color has been specified as white.\n",
    "\n",
    "#### Section 2\n",
    "* Create a figure from the word cloud object you generated in the previous step.\n",
    "* Display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate the word cloud from the east_of_eden string\n",
    "cloud_east_of_eden = WordCloud(background_color=\"white\").generate(east_of_eden)\n",
    "\n",
    "# Create a figure of the generated cloud\n",
    "plt.imshow(cloud_east_of_eden, interpolation='bilinear')  \n",
    "plt.axis('off')\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud on movie reviews\n",
    "\n",
    "You have been working with the movie reviews dataset. You have explored the distribution of the reviews and have seen how long the longest and the shortest reviews are. But what do positive and negative reviews talk about?\n",
    "\n",
    "In this exercise, you will practice building a word cloud of the top 100 positive reviews.\n",
    "\n",
    "What are the words that pop up? Do they make sense to you?\n",
    "\n",
    "The string `descriptions` has been created for you by concatenating the descriptions of the top 100 positive reviews. A movie-specific set of stopwords (very frequent words, such as the, a/an, and, which will not be very informative and we'd like to exclude from the graph) is available as `my_stopwords`. Recall that the interpolation argument makes the word cloud appear more smoothly.\n",
    "\n",
    "### Instructions\n",
    "* Import the `wordcloud` function from the respective package.\n",
    "* Apply the word cloud function to the descriptions string. Set the background color as `'white'`, and change the `stopwords` argument.\n",
    "* Create a wordcloud image.\n",
    "* Finally, do not forget to display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function  \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create and generate a word cloud image \n",
    "my_cloud = WordCloud(background_color='white', stopwords=my_stopwords).generate(descriptions)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Ch. 2 - Numeric Features from Reviews"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Your first BOW\n",
    "A bag-of-words is an approach to transform text to numeric form.\n",
    "\n",
    "In this exercise, you will apply a BOW to the annak list before moving on to a larger dataset in the next exercise.\n",
    "\n",
    "Your task will be to work with this list and apply a BOW using the `CountVectorizer()`. This transformation is your first step in being able to understand the sentiment of a text. Pay attention to words which might carry a strong sentiment.\n",
    "\n",
    "Remember that the output of a `CountVectorizer()` is a sparse matrix, which stores only entries which are non-zero. To look at the actual content of this matrix, we convert it to a dense array using the `.toarray()` method.\n",
    "\n",
    "Note that in this case you don't need to specify the max_features argument because the text is short.\n",
    "\n",
    "\n",
    "### Instructions\n",
    "* Import the count vectorizer function from `sklearn.feature_extraction.text`.\n",
    "* Build and fit the vectorizer on the small dataset.\n",
    "* Create the BOW representation with name `anna_bow` by calling the `transform()` method.\n",
    "* Print the BOW result as a dense array."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
    "\n",
    "# Build the vectorizer and fit it\n",
    "anna_vect = CountVectorizer()\n",
    "anna_vect.fit(annak)\n",
    "\n",
    "# Create the bow representation\n",
    "anna_bow = anna_vect.transform(annak)\n",
    "\n",
    "# Print the bag-of-words result \n",
    "print(anna_bow.toarray())"
   ]
  },
  {
   "source": [
    "## BOW using product reviews\n",
    "You practiced a BOW on a small dataset. Now you will apply it to a sample of Amazon product reviews. The data has been imported for you and is called reviews. It contains two columns. The first one is called `score` and it is 0 when the review is negative, and 1 when it is positive. The second column is called `review` and it contains the text of the review that a customer wrote. Feel free to explore the data in the IPython Shell.\n",
    "\n",
    "Your task is to build a BOW vocabulary, using the review column.\n",
    "\n",
    "Remember that we can call the `.get_feature_names()` method on the vectorizer to obtain a list of all the vocabulary elements.\n",
    "\n",
    "### Instructions\n",
    "* Create a CountVectorizer object, specifying the maximum number of features.\n",
    "* Fit the vectorizer.\n",
    "* Transform the fitted vectorizer.\n",
    "* Create a DataFrame where you transform the sparse matrix to a dense array and make sure to correctly specify the names of columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify max features \n",
    "vect = CountVectorizer(max_features=100)\n",
    "# Fit the vectorizer\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "source": [
    "## Specify token sequence length with BOW\n",
    "We saw in the video that by specifying different length of tokens - what we called n-grams - we can better capture the context, which can be very important.\n",
    "\n",
    "In this exercise, you will work with a sample of the Amazon product reviews. Your task is to build a BOW vocabulary, using the review column and specify the sequence length of tokens.\n",
    "\n",
    "Instructions\n",
    "* Build the vectorizer, specifying the token sequence length to be uni- and bigrams.\n",
    "* Fit the vectorizer.\n",
    "* Transform the fitted vectorizer.\n",
    "* In the DataFrame, make sure to correctly specify the column names."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify token sequence and fit\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "source": [
    "## Size of vocabulary of movies reviews\n",
    "In this exercise, you will practice different ways to limit the size of the vocabulary using a sample of the `movies` reviews dataset. The first column is the `review`, which is of type `object` and the second column is the `label`, which is 0 for a negative review and 1 for a positive one.\n",
    "\n",
    "The three methods that you will use will transform the text column to new numeric columns, capturing the count of a word or a phrase in each review. Each method will ultimately result in building a different number of new features.\n",
    "\n",
    "\n",
    "### Instructions \n",
    "#### Section 1\n",
    "* Using the movies dataset, limit the size of the vocabulary to 100.\n",
    "\n",
    "#### Section 2\n",
    "* Using the movies dataset, limit the size of the vocabulary to include terms which occur in no more than 200 documents.\n",
    "\n",
    "#### Section 3\n",
    "* Using the movies dataset, limit the size of the vocabulary to ignore terms which occur in less than 50 documents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify size of vocabulary and fit\n",
    "vect = CountVectorizer(max_features=100)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "\n",
    "# Section 2\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(max_df=200)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "\n",
    "# Section 3\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(min_df=50)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "source": [
    "## BOW with n-grams and vocabulary size\n",
    "In this exercise, you will practice building a bag-of-words once more, using the reviews dataset of Amazon product reviews. Your main task will be to limit the size of the vocabulary and specify the length of the token sequence.\n",
    "\n",
    "### Instructions\n",
    "* Import the vectorizer from `sklearn`.\n",
    "* Build the vectorizer and make sure to specify the following parameters: the size of the vocabulary should be limited to 1000, include only bigrams, and ignore terms that appear in more than 500 documents.\n",
    "* Fit the vectorizer to the `review` column.\n",
    "* Create a DataFrame from the BOW representation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Build the vectorizer, specify max features and fit\n",
    "vect = CountVectorizer(max_features=1000, ngram_range=(2, 2), max_df=500)\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create a DataFrame from the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "source": [
    "## Tokenize a string from GoT\n",
    "A first standard step when working with text is to tokenize it, in other words, split a bigger string into individual strings, which are usually single words (tokens).\n",
    "\n",
    "A string `GoT` has been created for you and it contains a quote from George R.R. Martin's Game of Thrones. Your task is to split it into individual tokens.\n",
    "\n",
    "### Instructions\n",
    "* Import the word tokenizing function from `nltk`.\n",
    "* Transform the GoT string to word tokens."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoT = 'Never forget what you are, for surely the world will not. Make it your strength. Then it can never be your weakness. Armour yourself in it, and it will never be used to hurt you.'\n",
    "\n",
    "# Import the required function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Transform the GoT string to word tokens\n",
    "print(word_tokenize(GoT))"
   ]
  },
  {
   "source": [
    "## Word tokens from the Avengers\n",
    "Now that you have tokenized your first string, it is time to iterate over items of a list and tokenize them as well. An easy way to do that with one line of code is with a list comprehension.\n",
    "\n",
    "A list `avengers` has been created for you. It contains a few quotes from the Avengers movies. You can explore it in the IPython Shell.\n",
    "\n",
    "### Instructions\n",
    "* Import the required function and package.\n",
    "* Apply the word tokenizing function on each item of our list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avengers = [\"Cause if we can't protect the Earth, you can be d*** sure we'll avenge it\", 'There was an idea to bring together a group of remarkable people, to see if we could become something more', \"These guys come from legend, Captain. They're basically Gods.\"]\n",
    "\n",
    "# Import the word tokenizing function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the avengers \n",
    "tokens_avengers = [word_tokenize(item) for item in avengers]\n",
    "\n",
    "print(tokens_avengers)"
   ]
  },
  {
   "source": [
    "## A feature for the length of a review\n",
    "You have now worked with a string and a list with string items, it is time to use a larger sample of data.\n",
    "\n",
    "You task in this exercise is to create a new feature for the length of a review, using the familiar reviews dataset.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Section 1\n",
    "* Import the word tokenizing function from the required package.\n",
    "* Apply the function to the `review` column of the `reviews` dataset.\n",
    "\n",
    "#### Section 2\n",
    "* Iterate over the created `word_tokens` list.\n",
    "* As you iterate, find the length of each item in the list and append it to the empty `len_tokens` list.\n",
    "* Create a new feature `n_words` in the reviews for the length of the reviews."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "# Import the needed packages\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the review column \n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "\n",
    "# Print out the first item of the word_tokens list\n",
    "print(word_tokens[0])\n",
    "\n",
    "# Section 2\n",
    "# Create an empty list to store the length of reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] = len_tokens"
   ]
  },
  {
   "source": [
    "## Identify the language of a string\n",
    "Sometimes you might need to analyze the sentiment of non-English text. Your first task in such a case will be to identify the foreign language.\n",
    "\n",
    "In this exercise you will identify the language of a single string. A string called `foreign` has been created for you. Feel free to explore it in the IPython Shell.\n",
    "\n",
    "### Instructions\n",
    "* Import the required function from the language detection package.\n",
    "* Detect the language of the `foreign` string."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreign = 'La histoire rendu étai fidèle, excellent, et grand.'\n",
    "\n",
    "# Import the language detection function and package\n",
    "from langdetect import detect_langs\n",
    "\n",
    "# Detect the language of the foreign string\n",
    "print(detect_langs(foreign))\n"
   ]
  },
  {
   "source": [
    "## Detect language of a list of strings\n",
    "Now you will detect the language of each item in a list. A list called `sentences` has been created for you and it contains 3 sentences, each in a different language. They have been randomly extracted from the product reviews dataset.\n",
    "\n",
    "### Instructions\n",
    "* Iterate over the sentences in the list.\n",
    "* Detect the language of each sentence and append the detected language to the empty list `languages`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "\n",
    "languages = []\n",
    "\n",
    "# Loop over the sentences in the list and detect their language\n",
    "for sentence in range(len(sentences)):\n",
    "    languages.append(detect_langs(sentences[sentence]))\n",
    "    \n",
    "print('The detected languages are: ', languages)"
   ]
  },
  {
   "source": [
    "## Language detection of product reviews\n",
    "You will practice language detection on a small dataset called `non_english_reviews`. It is a sample of non-English reviews from the Amazon product reviews.\n",
    "\n",
    "You will iterate over the rows of the dataset, detecting the language of each row and appending it to an empty list. The list needs to be cleaned so that it only contains the language of the review such as 'en' for English instead of the regular output `en:0.9987654`. Remember that the language detection function might detect more than one language and the first item in the returned list is the most likely candidate. Finally, you will assign the list to a new column.\n",
    "\n",
    "The logic is the same as used in the slides and the exercise before but instead of applying the function to a list, you work with a dataset.\n",
    "\n",
    "### Instructions\n",
    "* Iterate over the rows of the non_english_reviews dataset.\n",
    "* Inside the loop, detect the language of the second column of the dataset.\n",
    "* Clean the string by splitting on a : inside the list comprehension expression.\n",
    "* Finally, assign the cleaned list to a new column."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "languages = [] \n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "for row in range(len(non_english_reviews)):\n",
    "    languages.append(detect_langs(non_english_reviews.iloc[row, 1]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "\n",
    "# Assign the list to a new feature \n",
    "non_english_reviews['language'] = languages\n",
    "\n",
    "print(non_english_reviews.head())"
   ]
  },
  {
   "source": [
    "# Ch. 3 - More on Numeric Vectors: Transforming Tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Word cloud of tweets\n",
    "Your task in this exercise is to plot a word cloud using a sample of Twitter data, expressing customers' sentiments about airlines. A string `text_tweet` has been created for you and it contains the messages of a 1000 customers shared on Twitter.\n",
    "\n",
    "In the first step, your are asked to build the word cloud without removing the stop words, and in the second step to build the same cloud after you have removed the stop words.\n",
    "\n",
    "Feel free to familiarize yourself with the `text_tweet` list.\n",
    "\n",
    "### Instructions\n",
    "#### Section 1\n",
    "* Import the word cloud function and package.\n",
    "* Create and generate the word cloud, using the `text_tweet` vector.\n",
    "\n",
    "#### Section 2\n",
    "* Define the default list of stop words and update it.\n",
    "* Specify the stop words argument in the `WordCloud` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "# Import the word cloud function \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(background_color='white').generate(text_tweet)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Don't forget to show the final image\n",
    "plt.show()\n",
    "\n",
    "# Section 2\n",
    "# Import the word cloud function and stop words list\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "# Define and update the list of stopwords\n",
    "my_stop_words = set(STOPWORDS)\n",
    "my_stop_words = my_stop_words.update(['airline', 'airplane'])\n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(stopwords=my_stop_words).generate(text_tweet)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Airline sentiment with stop words\n",
    "You are given a dataset, called `tweets`, which contains customers' reviews and sentiments about airlines. It consists of two columns: `airline_sentiment` and `text` where the sentiment can be positive, negative or neutral, and the text is the text of the tweet.\n",
    "\n",
    "In this exercise, you will create a BOW representation but will account for the stop words. Remember that stop words are not informative and you might want to remove them. That will result in a smaller vocabulary and eventually, fewer features. Keep in mind that we can enrich a default list of stop words with ones that are specific to our context.\n",
    "\n",
    "Instructions\n",
    "* Import the default list of English stop words.\n",
    "* Update the default list of stop words with the given list `['airline', 'airlines', '@']` to create `my_stop_words`.\n",
    "* Specify the stop words argument in the vectorizer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the stop words\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@'])\n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(stop_words=my_stop_words)\n",
    "vect.fit(tweets.text)\n",
    "\n",
    "# Create the bow representation\n",
    "X_review = vect.transform(tweets.text)\n",
    "# Create the data frame\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "source": [
    "## Multiple text columns\n",
    "In this exercise, you will continue working with the airline Twitter data. A data set `tweets` has been imported for you.\n",
    "\n",
    "In some situations, you might have more than one text column in a dataset and you might want to create a numeric representation for each of the text columns. Here, besides the `text` column, which contains the body of the tweet, there is a second text column, called `negativereason`. It contains the reason the customer left a negative review.\n",
    "\n",
    "Your task is to build BOW representations for both columns and specify the required stop words.\n",
    "\n",
    "### Instructions\n",
    "* Import the vectorizer package and the default list of English stop words.\n",
    "* Update the default list of English stop words and create the `my_stop_words` set.\n",
    "* Specify the stop words argument in the first vectorizer to the updated set, and in the second vectorizer - the default set of English stop words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the vectorizer and default English stop words list\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the stop words\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@', 'am', 'pm'])\n",
    " \n",
    "# Build and fit the vectorizers\n",
    "vect1 = CountVectorizer(stop_words=my_stop_words)\n",
    "vect2 = CountVectorizer(stop_words=ENGLISH_STOP_WORDS) \n",
    "vect1.fit(tweets.text)\n",
    "vect2.fit(tweets.negative_reason)\n",
    "\n",
    "# Print the last 15 features from the first, and all from second vectorizer\n",
    "print(vect1.get_feature_names()[-15:])\n",
    "print(vect2.get_feature_names())"
   ]
  },
  {
   "source": [
    "## Specify the token pattern\n",
    "In this exercise, you will work with the `text` column of the tweets dataset. Your task is to vectorize the `object` column using `CountVectorizer`. You will apply different patterns of tokens in the vectorizer. Remember that by specifying the token pattern, you can filter out characters.\n",
    "\n",
    "The `CountVectorizer` has been imported for you.\n",
    "\n",
    "### Instructions\n",
    "#### Section 1\n",
    "* Build a vectorizer from the `text` column, specifying the pattern of tokens to be equal to `r'\\b[^\\d\\W][^\\d\\W]'`.\n",
    "\n",
    "#### Section 2\n",
    "* Build a vectorizer from the `text` column using the default values of the function's arguments.\n",
    "* Build a second vectorizer, specifying the pattern of tokens to be equal to `r'\\b[^\\d\\W][^\\d\\W]'`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]').fit(tweets.text)\n",
    "vect.transform(tweets.text)\n",
    "print('Length of vectorizer: ', len(vect.get_feature_names()))\n",
    "\n",
    "# Section 2\n",
    "# Build the first vectorizer\n",
    "vect1 = CountVectorizer().fit(tweets.text)\n",
    "vect1.transform(tweets.text)\n",
    "\n",
    "# Build the second vectorizer\n",
    "vect2 = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]').fit(tweets.text)\n",
    "vect2.transform(tweets.text)\n",
    "\n",
    "# Print out the length of each vectorizer\n",
    "print('Length of vectorizer 1: ', len(vect1.get_feature_names()))\n",
    "print('Length of vectorizer 2: ', len(vect2.get_feature_names()))"
   ]
  },
  {
   "source": [
    "## String operators with the Twitter data\n",
    "You continue working with the tweets data where the `text` column stores the content of each tweet.\n",
    "\n",
    "Your task is to turn the `text` column into a list of tokens. Then, using string operators, remove all non-alphabetic characters from the created list of tokens.\n",
    "\n",
    "### Instructions\n",
    "* Import the word tokenizing function.\n",
    "* Create word tokens from each tweet.\n",
    "* Filter out all non-alphabetic characters from the created list, i.e. retain only letters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word tokenizing package\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize the text column\n",
    "word_tokens = [word_tokenize(review) for review in tweets.text]\n",
    "print('Original tokens: ', word_tokens[0])\n",
    "\n",
    "# Filter out non-letter characters\n",
    "cleaned_tokens = [[word for word in item if word.isalpha()] for item in word_tokens]\n",
    "print('Cleaned tokens: ', cleaned_tokens[0])"
   ]
  },
  {
   "source": [
    "## More string operators and Twitter\n",
    "In this exercise, you will apply different string operators to three strings, selected from the `tweets` dataset. A `tweets_list` has been created for you.\n",
    "\n",
    "You need to construct three new lists by applying different string operators:\n",
    "\n",
    "* a list retaining only letters\n",
    "* a list retaining only characters\n",
    "* a list retaining only digits\n",
    "\n",
    "The required functions have been imported for you from `nltk`.\n",
    "\n",
    "### Instructions\n",
    "* Create a list of the tokens from `tweets_list`.\n",
    "* In the list letters remove all digits and other characters, i.e. keep only letters.\n",
    "* Retain alphanumeric characters but remove all other characters in `let_digits`.\n",
    "* Create digits by removing letters and characters and keeping only numbers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists, containing the tokens from list_tweets\n",
    "tokens = [word_tokenize(item) for item in tweets_list]\n",
    "\n",
    "# Remove characters and digits , i.e. retain only letters\n",
    "letters = [[word for word in item if word.isalpha()] for item in tokens]\n",
    "# Remove characters, i.e. retain only letters and digits\n",
    "let_digits = [[word for word in item if word.isalnum()] for item in tokens]\n",
    "# Remove letters and characters, retain only digits\n",
    "digits = [[word for word in item if word.isdigit()] for item in tokens]\n",
    "\n",
    "# Print the last item in each list\n",
    "print('Last item in alphabetic list: ', letters[2])\n",
    "print('Last item in list of alphanumerics: ', let_digits[2])\n",
    "print('Last item in the list of digits: ', digits[2])"
   ]
  },
  {
   "source": [
    "## Stems and lemmas from GoT\n",
    "In this exercise, you are given a couple of sentences from George R.R. Martin's Game of Thrones. Your task is to create stems and lemmas from the given `GoT` string.\n",
    "\n",
    "Remember that stems reduce a word to its root whereas lemmas produce an actual word. However, speed can differ significantly between the methods with stemming being much faster. In Steps 2 and 3, pay attention to the total time it takes to perform each operation. We're making use of the `time.time()` method to measure the time it takes to perform stemming and lemmatization.\n",
    "\n",
    "### Instructions\n",
    "#### Section 1\n",
    "* Import the stemming and lemmatization functions.\n",
    "* Build a list of tokens from the GoT string.\n",
    "\n",
    "#### Section 2\n",
    "* Using list comprehension and the `porter` stemmer you imported, create the `stemmed_tokens` list.\n",
    "\n",
    "#### Section 3\n",
    "* Using list comprehension and the `WNlemmatizer` you imported, create the `lem_tokens` list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "# Import the required packages from nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "porter = PorterStemmer()\n",
    "WNlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize the GoT string\n",
    "tokens = word_tokenize(GoT)\n",
    "\n",
    "# Section 2\n",
    "import time\n",
    "\n",
    "# Log the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build a stemmed list\n",
    "stemmed_tokens = [porter.stem(token) for token in tokens] \n",
    "\n",
    "# Log the end time\n",
    "end_time = time.time()\n",
    "\n",
    "print('Time taken for stemming in seconds: ', end_time - start_time)\n",
    "print('Stemmed tokens: ', stemmed_tokens) \n",
    "\n",
    "# Section 3\n",
    "import time\n",
    "\n",
    "# Log the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build a lemmatized list\n",
    "lem_tokens = [WNlemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Log the end time\n",
    "end_time = time.time()\n",
    "\n",
    "print('Time taken for lemmatizing in seconds: ', end_time - start_time)\n",
    "print('Lemmatized tokens: ', lem_tokens) \n"
   ]
  },
  {
   "source": [
    "## Stem Spanish reviews\n",
    "You will recall that in a previous chapter we used a language detection package to determine the language of different Amazon product reviews. In this exercise, you will first detect the languages in the `non_english_reviews` and then select only those in Spanish. Feel free to go back to the video discussing foreign language detection if you have forgotten some of the concepts.\n",
    "\n",
    "In the second step, you will create word tokens from the Spanish reviews and will stem them using a SnowBall stemmer for the Spanish language.\n",
    "\n",
    "### Instructions\n",
    "#### Section 1\n",
    "* Import the `langdetect` package.\n",
    "* Iterate over the rows of the `non_english_reviews` using the `len()` method and `range()` function.\n",
    "* Use `detect_langs()` to detect the language of each review in the `for` loop."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "# Import the language detection package\n",
    "import langdetect\n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "languages = [] \n",
    "for i in range(len(non_english_reviews)):\n",
    "    languages.append(langdetect.detect_langs(non_english_reviews.iloc[i, 1]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "# Assign the list to a new feature \n",
    "non_english_reviews['language'] = languages\n",
    "\n",
    "# Select the Spanish ones\n",
    "non_english_reviews = non_english_reviews[non_english_reviews.language == 'es']\n",
    "\n",
    "# Section 2\n",
    "# Import the required packages\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Import the Spanish SnowballStemmer\n",
    "SpanishStemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# Create a list of tokens\n",
    "tokens = [word_tokenize(review) for review in non_english_reviews.review] \n",
    "# Stem the list of tokens\n",
    "stemmed_tokens = [[SpanishStemmer.stem(word) for word in token] for token in tokens]\n",
    "\n",
    "# Print the first item of the stemmed tokenss\n",
    "print(stemmed_tokens[0])"
   ]
  },
  {
   "source": [
    "## Stems from tweets\n",
    "In this exercise, you will work with an array called `tweets`. It contains the text of the airline sentiment data collected from Twitter.\n",
    "\n",
    "Your task is to work with this array and transform it into a list of tokens using list comprehension. After that, iterate over the list of tokens and create a stem out of each token. Remember that list comprehensions are a one-line alternative to `for` loops.\n",
    "\n",
    "### Instructions\n",
    "* Import the function we used to transform strings into stems.\n",
    "* Call the Porter stemmer function you just imported.\n",
    "* Using a list comprehension, create the list tokens. It should contain all the word tokens from the `tweets` array.\n",
    "* Iterate over the `tokens` list and apply the stemming function to each item in the list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function to perform stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Call the stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Transform the array of tweets to tokens\n",
    "tokens = [word_tokenize(token) for token in tweets]\n",
    "# Stem the list of tokens\n",
    "stemmed_tokens = [[porter.stem(word) for word in tweet] for tweet in tokens] \n",
    "# Print the first element of the list\n",
    "print(stemmed_tokens[0])"
   ]
  },
  {
   "source": [
    "## Your first TfIdf\n",
    "In this exercise, you will apply the TfIdf method to the small `annak` dataset, containing the first sentence of Anna Karenina by Leo Tolstoy.\n",
    "\n",
    "Your task will be to work with this dataset and apply the `TfidfVectorizer()` function. Recall that performing a numeric transformation of text is your first step in being able to understand the sentiment of the text. The Tfidf vectorizer is another way to construct a vocabulary from our sentiment column.\n",
    "\n",
    "### Instructions\n",
    "* Import the function for building a Tfdif vectorizer from `sklearn.feature_extraction.text`.\n",
    "* Call the `TfidfVectorizer()` function and fit it on the `annak` dataset .\n",
    "* Transform the vectorizer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
    "\n",
    "# Call the vectorizer and fit it\n",
    "anna_vect = TfidfVectorizer().fit(annak)\n",
    "\n",
    "# Create the tfidf representation\n",
    "anna_tfidf = anna_vect.transform(annak)\n",
    "\n",
    "# Print the result \n",
    "print(anna_tfidf.toarray())"
   ]
  },
  {
   "source": [
    "## TfIdf on Twitter airline sentiment data\n",
    "You will now build features using the TfIdf method. You will continue to work with the `tweets` dataset.\n",
    "\n",
    "In this exercise, you will utilize what you have learned in previous lessons and remove stop words, use a token pattern and specify the n-grams.\n",
    "\n",
    "The final output will be a DataFrame, of which the columns are created using the `TfidfVectorizer()`. Such a DataFrame can directly be passed to a supervised learning model, which is what we will tackle in the next chapter.\n",
    "\n",
    "### Instructions\n",
    "* Import the required package to build a `TfidfVectorizer` and the `ENGLISH_STOP_WORDS`.\n",
    "* Build a TfIdf vectorizer from the `text` column of the `tweets` dataset, specifying uni- and bi-grams as a choice of n-grams, tokens which include only alphanumeric characters using the given token pattern, and the stop words corresponding to the `ENGLISH_STOP_WORDS`.\n",
    "* Transform the vectorizer, specifying the same column that you fit.\n",
    "* Specify the column names in the `DataFrame()` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required vectorizer package and stop words list\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the vectorizer and specify the arguments\n",
    "my_pattern = r'\\b[^\\d\\W][^\\d\\W]+\\b'\n",
    "vect = TfidfVectorizer(ngram_range=(1, 2), max_features=100, token_pattern=my_pattern, stop_words=ENGLISH_STOP_WORDS).fit(tweets.text)\n",
    "\n",
    "# Transform the vectorizer\n",
    "X_txt = vect.transform(tweets.text)\n",
    "\n",
    "# Transform to a data frame and specify the column names\n",
    "X=pd.DataFrame(X_txt.toarray(), columns=vect.get_feature_names())\n",
    "print('Top 5 rows of the DataFrame: ', X.head())"
   ]
  },
  {
   "source": [
    "## Tfidf and a BOW on same data\n",
    "In this exercise, you will transform the `review` column of the Amazon product `reviews` using both a bag-of-words and a tfidf transformation.\n",
    "\n",
    "Build both vectorizers, specifying only the maximum number of features to be equal to 100. Create DataFrames after the transformation and print the top 5 rows of each.\n",
    "\n",
    "### Instructions\n",
    "* Import the BOW and Tfidf vectorizers.\n",
    "* Build and fit a BOW and a Tfidf vectorizer from the `review` column and limit the number of created features to 100.\n",
    "* Create DataFrames from the transformed vector representations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "# Build a BOW and tfidf vectorizers from the review column and with max of 100 features\n",
    "vect1 = CountVectorizer(max_features=100).fit(reviews.review)\n",
    "vect2 = TfidfVectorizer(max_features=100).fit(reviews.review) \n",
    "\n",
    "# Transform the vectorizers\n",
    "X1 = vect1.transform(reviews.review)\n",
    "X2 = vect2.transform(reviews.review)\n",
    "# Create DataFrames from the vectorizers \n",
    "X_df1 = pd.DataFrame(X1.toarray(), columns=vect1.get_feature_names())\n",
    "X_df2 = pd.DataFrame(X2.toarray(), columns=vect2.get_feature_names())\n",
    "print('Top 5 rows using BOW: \\n', X_df1.head())\n",
    "print('Top 5 rows using tfidf: \\n', X_df2.head())"
   ]
  },
  {
   "source": [
    "# Ch. 4 - Let's Predict the Sentiment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Logistic regression of movie reviews\n",
    "In the video we learned that logistic regression is a common way to model a classification task, such as classifying the sentiment as positive or negative.\n",
    "\n",
    "In this exercise, you will work with the `movies` reviews dataset. The label `column` stores the sentiment, which is 1 when the review is positive, and 0 when negative. The `text` review has been transformed, using BOW, to numeric columns.\n",
    "\n",
    "Your task is to build a logistic regression model using the `movies` dataset and calculate its accuracy.\n",
    "\n",
    "### Instructions\n",
    "* Import the logistic regression function.\n",
    "* Create and fit a logistic regression on the labels `y` and the features `X`.\n",
    "* Calculate the accuracy of the logistic regression model, using the default `.score()` method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the vector of targets and matrix of features\n",
    "y = movies.label\n",
    "X = movies.drop('label', axis=1)\n",
    "\n",
    "# Build a logistic regression model and calculate the accuracy\n",
    "log_reg = LogisticRegression().fit(X, y)\n",
    "print('Accuracy of logistic regression: ', log_reg.score(X, y))"
   ]
  },
  {
   "source": [
    "## Logistic regression using Twitter data\n",
    "In this exercise, you will build a logistic regression model using the `tweets` dataset. The target is given by the `airline_sentiment`, which is 0 for negative tweets, 1 for neutral, and 2 for positive ones. So, in this case, you are given a multi-class classification task. Everything we learned about binary problems applies to multi-class classification problems as well.\n",
    "\n",
    "You will evaluate the accuracy of the model using the two different approaches from the slides.\n",
    "\n",
    "The logistic regression function and accuracy score have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "* Build and fit a logistic regression model using the defined `X` and `y` as arguments.\n",
    "* Calculate the accuracy of the logistic regression model.\n",
    "* Predict the labels.\n",
    "* Calculate the accuracy score using the predicted and true labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vector of targets and matrix of features\n",
    "y = tweets.airline_sentiment\n",
    "X = tweets.drop('airline_sentiment', axis=1)\n",
    "\n",
    "# Build a logistic regression model and calculate the accuracy\n",
    "log_reg = LogisticRegression().fit(X, y)\n",
    "print('Accuracy of logistic regression: ', log_reg.score(X, y))\n",
    "\n",
    "# Create an array of prediction\n",
    "y_predict = log_reg.predict(X)\n",
    "\n",
    "# Print the accuracy using accuracy score\n",
    "print('Accuracy of logistic regression: ', accuracy_score(y, y_predict))"
   ]
  },
  {
   "source": [
    "## Build and assess a model: movies reviews\n",
    "In this problem, you will build a logistic regression model using the movies dataset. The score is stored in the `label` column and is 1 when the review is positive, and 0 when negative. The text review has been transformed, using BOW, to numeric columns.\n",
    "\n",
    "You have already built a classifier but evaluated it using the same data employed in the training step. Make sure you now assess the model using an unseen test dataset. How does the performance of the model change when evaluated on the test set?\n",
    "\n",
    "Instructions\n",
    "* Import the function required for a train/test split.\n",
    "* Perform the train/test split, specifying that 20% of the data should be used as a test set.\n",
    "* Train a logistic regression model.\n",
    "* Print out the accuracy of the model on the training and on the testing data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the vector of labels and matrix of features\n",
    "y = movies.label\n",
    "X = movies.drop('label', axis=1)\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a logistic regression model and print out the accuracy\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "print('Accuracy on train set: ', log_reg.score(X_train, y_train))\n",
    "print('Accuracy on test set: ', log_reg.score(X_test, y_test))"
   ]
  },
  {
   "source": [
    "## Performance metrics of Twitter data\n",
    "You will train a logistic regression model that predicts the sentiment of tweets and evaluate its performance on the test set using different metrics.\n",
    "\n",
    "A matrix `X` has been created for you. It contains features created with a BOW on the `text` column.\n",
    "\n",
    "The labels are stored in a vector called `y`. Vector `y` is 0 for negative tweets, 1 for neutral, and 2 for positive ones.\n",
    "Note that although we have 3 classes, it is still a classification problem. The accuracy still measures the proportion of correctly predicted instances. The confusion matrix will now be of size 3x3, each row will give the number of predicted cases for classes 2, 1, and 0, and each column - the true number of cases in class 2, 1, and 0.\n",
    "\n",
    "All required packages have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "* Perform the train/test split, and stratify by `y`.\n",
    "* Train a a logistic regression classifier.\n",
    "* Predict the performance on the test set.\n",
    "* Print the accuracy score and confusion matrix obtained on the test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify=y)\n",
    "\n",
    "# Train a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_predicted = log_reg.predict(X_test)\n",
    "\n",
    "# Print the performance metrics\n",
    "print('Accuracy score test set: ', accuracy_score(y_test, y_predicted))\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_predicted)/len(y_test))"
   ]
  },
  {
   "source": [
    "## Build and assess a model: product reviews data\n",
    "In this exercise, you will build a logistic regression using the `reviews` dataset, containing customers' reviews of Amazon products. The array `y` contains the sentiment : 1 if positive and 0 otherwise. The array `X` contains all numeric features created using a BOW approach. Feel free to explore them in the IPython Shell.\n",
    "\n",
    "Your task is to build a logistic regression model and calculate the accuracy and confusion matrix using the test data set.\n",
    "\n",
    "The logistic regression and train/test splitting functions have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "* Import the accuracy score and confusion matrix functions.\n",
    "* Split the data into training and testing, using 30% of it as a test set and set the random seed to 42.\n",
    "* Train a logistic regression model.\n",
    "* Print out the accuracy score and confusion matrix using the test data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the accuracy and confusion matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels \n",
    "y_predict = log_reg.predict(X_test)\n",
    "\n",
    "# Print the performance metrics\n",
    "print('Accuracy score of test data: ', accuracy_score(y_test, y_predict))\n",
    "print('Confusion matrix of test data: \\n', confusion_matrix(y_test, y_predict)/len(y_test))"
   ]
  },
  {
   "source": [
    "## Predict probabilities of movie reviews\n",
    "In this problem, you will build a logistic regression using the `movies` dataset. The labels are stored in the array `y` and the features in `X`.\n",
    "\n",
    "Train the model on the training data. Instead of predicting classes, predict the probabilities that each instance in the test set belongs to each of the two classes.\n",
    "\n",
    "The logistic regression and train/test splitting functions have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "* Split the data into training and testing set.\n",
    "* Train a logistic regression model.\n",
    "* Predict the probabilities for class 0 and for class 1 of the testing data. Class 0 is located as the first column in the predicted probabilities, and class 1 is the second one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321)\n",
    "\n",
    "# Train a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predict the probability of the 0 class\n",
    "prob_0 = log_reg.predict_proba(X_test)[:, 0]\n",
    "# Predict the probability of the 1 class\n",
    "prob_1 = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"First 10 predicted probabilities of class 0: \", prob_0[:10])\n",
    "print(\"First 10 predicted probabilities of class 1: \", prob_1[:10])"
   ]
  },
  {
   "source": [
    "## Product reviews with regularization\n",
    "In this exercise, you will work once more with the `reviews` dataset of Amazon product reviews. A vector of labels `y` contains the sentiment: 1 if positive and 0 otherwise. The matrix `X` contains all numeric features created using a BOW approach.\n",
    "\n",
    "You will need to train two logistic regression models with different levels of regularization and compare how they perform on the test data. Remember that regularization is a way to control the complexity of the model. The more regularized a model is, the less flexible it is but the better it can generalize. Models with higher level of regularization are often less accurate than non-regularized ones.\n",
    "\n",
    "### Instructions\n",
    "* Split the data into a train and test sets.\n",
    "* Train a logistic regression with regularization parameter of 1000. Train a second logistic regression with regularization parameter equal to 0.001.\n",
    "* Print the accuracy scores of both models on the test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Train a logistic regression with regularization of 1000\n",
    "log_reg1 = LogisticRegression(C=1000).fit(X_train, y_train)\n",
    "# Train a logistic regression with regularization of 0.001\n",
    "log_reg2 = LogisticRegression(C=0.001).fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracies\n",
    "print('Accuracy of model 1: ', log_reg1.score(X_test, y_test))\n",
    "print('Accuracy of model 2: ', log_reg2.score(X_test, y_test))"
   ]
  },
  {
   "source": [
    "## Regularizing models with Twitter data\n",
    "You will work with the Twitter data expressing customers' sentiment about airline companies. The `X` matrix of features and `y` vector of labels have been created for you. In addition, the training and testing split has been performed. You can work with the `X_train`, `X_test`, `y_train` and `y_test` arrays directly.\n",
    "\n",
    "You will train regularized and a more flexible models and evaluate them using different model performance metrics.\n",
    "\n",
    "All required packages have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "* Train two logistic regressions: one with regularization parameter of 100 and a second of 0.1.\n",
    "* Print the accuracy scores of both models.\n",
    "* Print the confusion matrix of each model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a logistic regression with regularizarion parameter of 100\n",
    "log_reg1 = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "# Build a logistic regression with regularizarion parameter of 0.1\n",
    "log_reg2 = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for each model\n",
    "y_predict1 = log_reg1.predict(X_test)\n",
    "y_predict2 = log_reg2.predict(X_test)\n",
    "\n",
    "# Print performance metrics for each model\n",
    "print('Accuracy of model 1: ', log_reg1.score(X_test, y_test))\n",
    "print('Accuracy of model 2: ', log_reg2.score(X_test, y_test))\n",
    "print('Confusion matrix of model 1: \\n' , confusion_matrix(y_test, y_predict1)/len(y_test))\n",
    "print('Confusion matrix of model 2: \\n', confusion_matrix(y_test, y_predict2)/len(y_test))"
   ]
  },
  {
   "source": [
    "## Step 1: Word cloud and feature creation\n",
    "You will work with a sample of the reviews dataset throughout this exercise. It contains the `review` and `score` columns. Feel free to explore it in the IPython Shell.\n",
    "\n",
    "In the first step, you will build a word cloud using only positive reviews. The string `positive_reviews` has been created for you by concatenating the top 100 positive reviews.\n",
    "\n",
    "In the second step, you will create a new feature for the length of each review and add that new feature to the dataset.\n",
    "\n",
    "All the functions needed to plot a word cloud have been imported for you, as well as the `word_tokenize` function from the `nltk` module.\n",
    "\n",
    "### Instructions\n",
    "#### Section 1\n",
    "* Call and create a word cloud image using the positive_reviews.\n",
    "* Display the generated image.\n",
    "\n",
    "#### Section 2\n",
    "* Tokenize each item in the `review` column, using the word tokenizing function we have been working with.\n",
    "* Iterate over the created `word_tokens` list and find the length of each item in the list. Append that length to the empty `len_tokens` list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "# Create and generate a word cloud image\n",
    "cloud_positives = WordCloud(background_color='white').generate(positive_reviews)\n",
    " \n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(cloud_positives, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Don't forget to show the final image\n",
    "plt.show()\n",
    "\n",
    "# Section 2\n",
    "# Tokenize each item in the review column\n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "\n",
    "# Create an empty list to store the length of the reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] = len_tokens "
   ]
  },
  {
   "source": [
    "## Step 2: Building a vectorizer\n",
    "In this exercise, you are asked to build a TfIDf transformation of the `review` column in the `reviews` dataset. You are asked to specify the n-grams, stop words, the pattern of tokens and the size of the vocabulary arguments.\n",
    "\n",
    "This is the last step before we train a classifier to predict the sentiment of a review.\n",
    "\n",
    "Instructions\n",
    "* Import the Tfidf vectorizer and the default list of English stop words.\n",
    "* Build the Tfidf vectorizer, specifying - in this order - the following arguments: use as stop words the default list of English stop words; as n-grams use uni- and bi-grams;the maximum number of features should be 200; capture only words using the specified pattern.\n",
    "* Create a DataFrame using the Tfidf vectorizer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TfidfVectorizer and default list of English stop words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Build the vectorizer\n",
    "vect = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 2), max_features=200, token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b').fit(reviews.review)\n",
    "# Create sparse matrix from the vectorizer\n",
    "X = vect.transform(reviews.review)\n",
    "\n",
    "# Create a DataFrame\n",
    "reviews_transformed = pd.DataFrame(X.toarray(), columns=vect.get_feature_names())\n",
    "print('Top 5 rows of the DataFrame: \\n', reviews_transformed.head())"
   ]
  },
  {
   "source": [
    "## Step 3: Building a classifier\n",
    "This is the last step in the sentiment analysis prediction. We have explored and enriched our dataset with features related to the sentiment, and created numeric vectors from it.\n",
    "\n",
    "You will use the dataset that you built in the previous steps. Namely, it contains a feature for the length of reviews, and 200 features created with the Tfidf vectorizer.\n",
    "\n",
    "Your task is to train a logistic regression to predict the sentiment. The data has been imported for you and is called `reviews_transformed`. The target is called `score` and is binary: 1 when the product review is positive and 0 otherwise.\n",
    "\n",
    "Train a logistic regression model and evaluate its performance on the test data. How well does the model do?\n",
    "\n",
    "All the required packages have been imported for you.\n",
    "\n",
    "### Instructions\n",
    "* Perform the train/test split, allocating 20% of the data to testing and setting the random seed to 456.\n",
    "* Train a logistic regression model.\n",
    "* Predict the class.\n",
    "* Print out the accuracy score and the confusion matrix on the test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "y = reviews_transformed.score\n",
    "X = reviews_transformed.drop('score', axis=1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=456)\n",
    "\n",
    "# Train a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "# Predict the labels\n",
    "y_predicted = log_reg.predict(X_test)\n",
    "\n",
    "# Print accuracy score and confusion matrix on test set\n",
    "print('Accuracy on the test set: ', log_reg.score(X_test, y_test))\n",
    "print(confusion_matrix(y_test, y_predicted)/len(y_test))"
   ]
  }
 ]
}